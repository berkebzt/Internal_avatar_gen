
"""akbankavatargenerator.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1P-zv_YXExbDAWqTXPgXjh3QDcrC-XlWr
"""

#####################################
import imageio
import streamlit as st
import base64
import torch
from PIL import Image
#####################################
# Functions:

# Function for calling any of the two stable diffusion models, imports and deletes required modules, parameters are self explanatory, returns the image:
def stablediffusioncall(option, user_prompt, user_n_prompt, i_inference_steps, i_initial_seed, i_guidance_scale, situation):
  from diffusers import StableDiffusionPipeline

  if option == "Stable Diffusion v1-4":        # Uses a PNDM scheduler
    model_id = "CompVis/stable-diffusion-v1-4"
  elif option == "Stable Diffusion 2-1":
    model_id = "stabilityai/stable-diffusion-2-1" # Uses a DDIM scheduler

  device = "cuda"
  pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)
  pipe = pipe.to(device)

  prompt = user_prompt
  negative_prompt = user_n_prompt

  if situation:     # If custom parameters are chosen, parameters are given the user input values. If not, the pipeline is called without change.
    num_inference_steps = i_inference_steps
    guidance_scale = i_guidance_scale

    if i_initial_seed is not None:        # In case of wanting to use custom seed, use the user input value. If not the pipeline is called without seed input.
      generator = torch.Generator("cuda").manual_seed(i_initial_seed)
      image = pipe(prompt=prompt, num_inference_steps=num_inference_steps, guidance_scale=guidance_scale, negative_prompt=negative_prompt, generator=generator ).images[0]
    else:
      image = pipe(prompt=prompt, num_inference_steps=num_inference_steps, guidance_scale=guidance_scale, negative_prompt=negative_prompt).images[0]
  else:
    image = pipe(prompt=prompt, negative_prompt=negative_prompt).images[0]

  del StableDiffusionPipeline
  return image

# Function for calling FuseDream model, imports and deletes required modules, parameters are self explanatory, returns the image:
def fusedreamcall(user_prompt, INIT_ITERS, OPT_ITERS, NUM_BASIS, SEED, use_seed):
  from tqdm import tqdm
  from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize
  import torchvision
  import BigGAN_utils.utils as utils
  import clip
  import torch.nn.functional as F
  from DiffAugment_pytorch import DiffAugment
  import numpy as np
  from fusedream_utils import FuseDreamBaseGenerator, get_G
  import sys

  sentence = user_prompt

  if use_seed:           # If the function is called using default parameter values SEED and use_seed are given the value None. If the utils.seed_rng(SEED) is called with None it gives an error, therefore the if statment is used.
    utils.seed_rng(SEED)

  sys.argv = ['']

  G, config = get_G(512)
  generator = FuseDreamBaseGenerator(G, config, 10)
  z_cllt, y_cllt = generator.generate_basis(sentence, init_iters=INIT_ITERS, num_basis=NUM_BASIS)

  z_cllt_save = torch.cat(z_cllt).cpu().numpy()
  y_cllt_save = torch.cat(y_cllt).cpu().numpy()
  img, z, y = generator.optimize_clip_score(z_cllt, y_cllt, sentence, latent_noise=False, augment=True, opt_iters=OPT_ITERS, optimize_y=True)

  image = img
  image = (image / 2 + 0.5).clamp(0, 1)
  image = image.detach().cpu().permute(0, 2, 3, 1).numpy()
  images = (image * 255).round().astype("uint8")
  pil_images = [Image.fromarray(image) for image in images]
  image = pil_images[0]

  del tqdm
  del torchvision
  del utils
  del clip
  del F
  del DiffAugment
  del FuseDreamBaseGenerator
  del get_G
  return image

# Function for fine-tuning stable diffusion v1-5 model, imports and deletes required modules, parameters are self explanatory, returns the fine-tuned model pipeline:
def finetunesdcall(u_token, u_class, plant, t_b_size, l_r, n_c_images, s_b_size, m_t_steps, uploaded_files):
  # Settings (including model selection):

  import subprocess

  s_prompt=u_token+" "+u_class

  MODEL_NAME = "runwayml/stable-diffusion-v1-5"

  OUTPUT_DIR = "/content/stable_diffusion_weights/" + u_token

  try:
      # Construct the shell command
      command = f"mkdir -p {OUTPUT_DIR}"
      # Execute the shell command
      subprocess.run(command, shell=True, check=True)
  except subprocess.CalledProcessError as e:
      # Handle any errors that occur during command execution
      print(f"Error: {e}")

  # Choosing the instance and class prompts:

  # You can also add multiple concepts here, try tweaking `--max_train_steps` accordingly.
  # `class_data_dir` contains the regularization images
  concepts_list = [
      {
          "instance_prompt":      f"photo of {u_token} man",
          "class_prompt":         "photo of a man",
          "instance_data_dir":    "/content/data/" + u_token,
          "class_data_dir":       "/content/data/man"
      },
      {
          "instance_prompt":      f"photo of {u_token} woman",
          "class_prompt":         "photo of a woman",
          "instance_data_dir":    "/content/data/" + u_token,
          "class_data_dir":       "/content/data/woman"
      },
      {
          "instance_prompt":      f"photo of {u_token} person",
          "class_prompt":         "photo of a person",
          "instance_data_dir":    "/content/data/" + u_token,
          "class_data_dir":       "/content/data/person"
      }
  ]

  import json
  import os
  for c in concepts_list:
      os.makedirs(c["instance_data_dir"], exist_ok=True)

  keep_cl = []
  for c in concepts_list:
    a_class = c['instance_prompt'].split()
    if a_class[-1] == u_class:
      keep_cl.append(c)

  with open("concepts_list.json", "w") as f:
      json.dump(keep_cl, f, indent=4)

  # Upload your images

  from google.colab import files
  import shutil

  for c in concepts_list:
    a_class = c['instance_prompt'].split()
    if a_class[-1] == u_class:
      uploaded = uploaded_files
      for element in uploaded:
        filename = element.name
        dst_path = os.path.join(c['instance_data_dir'], filename)
        with open(dst_path, "wb") as f:
            f.write(element.read())

  # Fine-tuning the SD model:

  # Tweak the parameters for desired image quality --> 1200 training steps = 40 newly introdued images for 30 epochs.
  command = f"""
          python3 train_dreambooth.py \
          --pretrained_model_name_or_path={MODEL_NAME} \
          --pretrained_vae_name_or_path="stabilityai/sd-vae-ft-mse" \
          --output_dir={OUTPUT_DIR} \
          --revision="fp16" \
          --with_prior_preservation --prior_loss_weight=1.0 \
          --seed={plant} \
          --resolution=512 \
          --train_batch_size={t_b_size} \
          --train_text_encoder \
          --mixed_precision="fp16" \
          --use_8bit_adam \
          --gradient_accumulation_steps=1 \
          --learning_rate={l_r} \
          --lr_scheduler="constant" \
          --lr_warmup_steps=0 \
          --num_class_images={n_c_images} \
          --sample_batch_size={s_b_size} \
          --max_train_steps={m_t_steps} \
          --save_interval=10000 \
          --save_sample_prompt="{s_prompt}" \
          --concepts_list="concepts_list.json"
      """
  try:
      # Execute the shell command
      subprocess.run(command, shell=True, check=True, executable="/bin/bash")
  except subprocess.CalledProcessError as e:
      # Handle any errors that occur during command execution
      print(f"Error: {e}")

  # Reduce the `--save_interval` to lower than `--max_train_steps` to save weights from intermediate steps.
  # `--save_sample_prompt` can be same as `--instance_prompt` to generate intermediate samples (saved along with weights in samples)

  # Specify the weights directory to use (leave blank for latest):

  from natsort import natsorted
  from glob import glob
  WEIGHTS_DIR = natsorted(glob(OUTPUT_DIR + os.sep + "*"))[-1]

  # Inference:

  import torch
  from torch import autocast
  from diffusers import StableDiffusionPipeline, DDIMScheduler
  from IPython.display import display

  model_path = WEIGHTS_DIR

  pipe = StableDiffusionPipeline.from_pretrained(model_path, safety_checker=None, torch_dtype=torch.float16).to("cuda")
  pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)
  pipe.enable_xformers_memory_efficient_attention()
  g_cuda = None

  del json
  del os
  del files
  del shutil
  del natsorted
  del glob
  del torch
  del autocast
  del display
  del subprocess
  return pipe

# Function for generating prompt conditioned images with fine-tuned stable diffusion v1-5 model, imports and deletes required modules, parameters are self explanatory, returns the image:
def dbgeneratecall(pipe, user_prompt, user_n_prompt, guidance_scale, num_inference_steps, use_seed, s33d):

  import torch
  from torch import autocast
  from diffusers import StableDiffusionPipeline, DDIMScheduler

  # Run for generating images.

  prompt = user_prompt
  negative_prompt = user_n_prompt
  num_samples = 1
  height = 512
  width = 512

  if use_seed:
    g_cuda = torch.Generator(device='cuda')
    seed = s33d
    g_cuda.manual_seed(seed)
    with autocast("cuda"), torch.inference_mode():
        images = pipe(
            prompt,
            height=height,
            width=width,
            negative_prompt=negative_prompt,
            num_images_per_prompt=num_samples,
            num_inference_steps=num_inference_steps,
            guidance_scale=guidance_scale,
            generator=g_cuda
        ).images
  else:
    with autocast("cuda"), torch.inference_mode():
        images = pipe(
            prompt,
            height=height,
            width=width,
            negative_prompt=negative_prompt,
            num_images_per_prompt=num_samples,
            num_inference_steps=num_inference_steps,
            guidance_scale=guidance_scale,
        ).images

  for img in images:
    generated = img

  del torch
  del autocast
  return generated

###############################################################################################################

def main():

  icon = Image.open('./images/icon.png')
  st.set_page_config(page_icon = icon, layout="wide")

  #background_image = Image.open('/content/background.jpg')

  col1, coli1, col2, coli2, col3, = st.columns((1,0.25,2,0.25,1))

  image = Image.open('./images/akbank.jpg')
  image2 = Image.open('./images/sabanci.png')

  if 'files_uploaded' not in st.session_state:  # Keeps track of if any image has been uploaded (keeps boolean value). Initializing it beforehand, prevents being called before being initialized error
    st.session_state.files_uploaded = False

  if 'pictures' not in st.session_state:  # Keeps track of if any image has been uploaded (keeps the pictures themselves). Initializing it beforehand, prevents being called before being initialized error
    st.session_state.pictures = False

  if 'pipe_s' not in st.session_state:  # Keeps track of if pipe_s has been returned by finetunesdcall (keeps the pipeline). Initializing it beforehand, prevents being called before being initialized error
    st.session_state.pipe_s = False

  if 'fine_tuned' not in st.session_state:  # Keeps track of if the model has been fine-tuned. Initializing it beforehand, prevents being called before being initialized error
    st.session_state.fine_tuned = False

  with col1:
    st.image(image, caption='', width = 500, use_column_width=True)
    with st.sidebar:
      st.title(':red[MODELS]')
      st.header('General model information', divider='red')

      with st.expander("Stable Diffusion v1-4"):
        st.write("PLaceholder.")

      with st.expander("Stable Diffusion 2-1"):
        st.write("PLaceholder.")

      with st.expander("FuseDream (CLIP+BigGAN)"):
        st.write("PLaceholder.")

      with st.expander("Non-specified Flow based model"):
        st.write("PLaceholder.")

      st.header('Settings to tweak the models', divider='red')
      # situation = True --> Want to use custom parameters, situation = False --> Want to use default parameters
      situation = st.toggle('Use custom parameters for models')
      if situation:
        with st.expander('Diffusion model parameters:'):  # Have 3 parameters

          i_inference_steps = st.number_input('Number of inference steps', step = 1)

          use_seed = st.toggle('Enter a seed value', help = 'If seed value not entered, each generated image will be given a random one.')
          if use_seed:
            i_initial_seed = st.number_input('Initial seed', value = 0, step = 1)
          else:
            i_initial_seed = None

          i_guidance_scale = st.slider('Guidance scale', 1.0, 25.0, 7.5, step = 0.5)

        with st.expander('FuseDream (CLIP+BigGAN) parameters:'): # Has 5 parameters (including the boolean use_seed)

          INIT_ITERS = st.number_input('Number of images used for initialization', step = 1)

          OPT_ITERS = st.number_input('Number of iterations', step = 1)

          NUM_BASIS = st.number_input('Number of basis images', step = 1)

          use_seed = st.toggle('Enter a seed value', key = 4, help = 'If seed value not entered, each generated image will be given a random one.')
          if use_seed:
            SEED = st.number_input('Initial seed', value = 0, step = 1, key = 5)
          else:
            SEED = None

        with st.expander('Non-specified Flow based model parameters:'): # Not yet implemented, has 0 parameters
          st.write("PLaceholder.")

      st.header(' ', divider='red')
      # Dreambooth is separated from the situation block since it should not be called with default parameters as the parameters need to be entered according to the external data and its use. (TLDR Attune parameters per each use for good results.)
      with st.expander('Dreambooth parameters:'): # Has 8 parameters for fine-tuning and 4 parameters for image generation (including use_seed boolean)

        u_token = st.text_input('Unique token', '', placeholder='Enter a unique token.', help = 'example: name_surname or zwx')

        u_class = st.text_input('Class of reg images', '', placeholder='Enter a regularization image set class.', help = 'Available class names: man, woman, person')

        plant = st.number_input('Initial seed', value = 0, step = 1, key = 3)

        t_b_size = st.number_input('Training bach size', step =1, key = 1)

        l_r = st.slider('Learning rate', 1, 6, step = 1)
        l_r = l_r * 1e-6

        n_c_images = st.number_input('Number of class images', step =1)

        s_b_size = st.number_input('Sample batch size', step =1)

        m_t_steps = st.number_input('Training steps', step =1)

        ft_start = st.button('Start fine-tuning')

        if ft_start and st.session_state.files_uploaded:   # Checks if any photos have been uploaded before starting fine-tuning, if not gives an error
          process = st.empty()            # Creating a Fine-tuning...
          process.text('Fine-tuning...')

          st.session_state.pipe_s = finetunesdcall(u_token, u_class, plant, t_b_size, l_r, n_c_images, s_b_size, m_t_steps, st.session_state.pictures)              # Calls dreambooth fine-tuning function
          st.session_state.fine_tuned = True

          process.empty()        # Deletes Fine-tuning... text
          st.write('Fine-tuning completed, now an image can be generated')
        elif ft_start and not st.session_state.files_uploaded:
          st.error('Please upload images to fine-tune the model', icon="🚨")

        guidance_scale = st.slider('Guidance scale', 1.0, 25.0, 7.5, step = 0.5, key = 12)

        num_inference_steps = st.number_input('Number of inference steps', step = 1, key = 13)

        use_seed = st.toggle('Enter a seed value', key = 14, help = 'If seed value not entered, each generated image will be given a random one.')
        if use_seed:
          s33d = st.number_input('Initial seed', value = 0, step = 1, key = 5)
        else:
          s33d = None

  with col2:
    st.title("Akbank Avatar Generator")
    use_db = st.toggle('Use Dreambooth')  # If you want to use Dreambooth, reveals the following untill the line:
    if use_db:
      st.write('Show us what you look like 😊')
      uploaded_files = st.file_uploader("Upload your photos here...", type=['png', 'jpg', 'jpeg'], accept_multiple_files=True, help = 'Please only upload JPEGs or PNGs of 512x512 pixels.')
      if uploaded_files:
        st.session_state.pictures = uploaded_files
        st.session_state.files_uploaded = True
      information = st.checkbox('See additional information for fine tuned image generation with Dreambooth')
      if information:               # Additional infromation toggle
        display_text = """
        Ideal parameters for cartoonish avatar generation:

        (Used 8 inputted training images)
        ▪️ Training batch size = 2 (effect not tested)
        ▪️ Learning rate = 1e-6
        ▪️ Number of class images = 100 (effect not tested)
        ▪️ Sample batch size = 4 (effect not tested)
        ▪️ Training steps = 640 and 664
          (The ideal number of training steps seems to be 80-83 epochs per image.)
        ▪️ Sample prompt = unique token + class of regularization images
        ▪️ Prompt = A cartoonish avatar version of Sample prompt.
        ▪️ Negative prompt = Up to the user
        ▪️ Guidance scale = 9
        ▪️ Number of inference steps = 100
        """
        st.text_area("Ideal parameters for different prompts and uses:", display_text)

      st.write(':red[Attention:] Dreambooth is currently only available for Stable-Diffusion-v1-5. When using Dreambooth this model is automatically implemented, no need to choose models 😅')
################################################################################
    option = st.selectbox('What model would you like to use?', ('Stable Diffusion v1-4', 'Stable Diffusion 2-1', 'FuseDream (CLIP+BigGAN)', 'Non-specified Flow based model'), placeholder = "Select your model...")

    user_prompt = st.text_input('Prompt', '', placeholder='* Required to fill this area.')

    user_n_prompt = st.text_input('Negative prompt (Optional)', '')
    st.write(':red[Attention:] Negative prompts are currently only available for diffusion models')

    generation_start = st.button('Start generating')
    if generation_start and user_prompt == '':          # Ensures a prompt is entered
      st.error('Please enter a prompt', icon="🚨")
    elif generation_start and user_prompt != '':

      select_sd = option == 'Stable Diffusion v1-4' or option == 'Stable Diffusion 2-1'

      process = st.empty()            # Creating a Generating...
      process.text('Generating...')

      if use_db:        # Selected model is Dreambooth fine tuning on Stable Diffusion v1-5
        if st.session_state.fine_tuned:
          generated_image = dbgeneratecall(st.session_state.pipe_s, user_prompt, user_n_prompt, guidance_scale, num_inference_steps, use_seed, s33d)

          process.empty()            # Deletes Generating... text
          st.write('')

          st.image(generated_image, caption= 'Generated image using Dreambooth fine-tuning.')       # Display the generated image with caption
        else:
          process.empty()            # Deletes Generating... text
          st.error('Please first fine-tune the model', icon="🚨")

      elif select_sd:       # Selected model is a Stable Diffusion one
        if not situation:
          i_inference_steps = 0         # If chosen to use default parameters, gives placeholder values that will be changed to default values within the stablediffusioncall function
          i_guidance_scale = 0
          i_initial_seed = 0

        generated_image = stablediffusioncall(option, user_prompt, user_n_prompt, i_inference_steps, i_initial_seed ,i_guidance_scale, situation)       # Calls stablediffusioncall function

        process.empty()            # Deletes Generating... text
        st.write('')

        st.image(generated_image, caption= f'Generated image using {option}.')       # Display the generated image with caption

      elif option == 'FuseDream (CLIP+BigGAN)':    # Selected model is FuseDream
        if not situation:
          INIT_ITERS =  1000
          OPT_ITERS = 1000                 # If chosen to use default parameters, gives default values
          NUM_BASIS = 10
          SEED = None
          use_seed = False

        generated_image = fusedreamcall(user_prompt, INIT_ITERS, OPT_ITERS, NUM_BASIS, SEED, use_seed)            # Calls fusedreamcall function

        process.empty()           # Deletes Generating... text
        st.write('')

        st.image(generated_image, caption= f'Generated image using {option}.')    # Display the generated image with caption

      elif option == 'Non-specified Flow based model':      # Selected model is a Non-specified Flow based model
        process.empty()                  # Deletes Generating... text
        st.write('')

        st.write('Error: This model is not implemented yet', key = 51)

  with col3:
    st.image(image2, caption='', width = 150, use_column_width=True, output_format="PNG")

###############################################################################################################

if __name__ == "__main__": 
    main()