{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MP5fXsaWu5gs"
   },
   "source": [
    "# DOCUMENTATION AND NOTES:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UrW_QDUiNFn6"
   },
   "source": [
    "*** Everything used: ***\n",
    "\n",
    "Streamlit will be used to build the app: https://streamlit.io\n",
    "\n",
    "Diffusion model pretrained pipelines:\n",
    "* https://huggingface.co/CompVis/stable-diffusion-v1-4\n",
    "* https://huggingface.co/stabilityai/stable-diffusion-2-1\n",
    "\n",
    "StudioGAN will be used for pretrained GAN models: https://github.com/POSTECH-CVLab/PyTorch-StudioGAN (Not used)\n",
    "\n",
    "One of the GAN models is FuseDream (CLIP + BigGAN):\n",
    " * article: https://analyticsindiamag.com/fusedream-a-hands-on-tutorial-on-this-text-to-image-generation-tool/\n",
    " * author repo: https://github.com/gnobitab/FuseDream\n",
    " * BigGAN repo: https://github.com/huggingface/pytorch-pretrained-BigGAN\n",
    "\n",
    "Dreambooth:\n",
    "* Used BIRME for resizing images to 512x512: https://www.birme.net/?target_width=512&target_height=512\n",
    "\n",
    "* Used Dreambooth by Shivam Shrirao diffuser based repo: https://github.com/ShivamShrirao/diffusers/tree/main/examples/dreambooth (examples/dreambooth/DreamBooth_Stable_Diffusion.ipynb)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RohR3cjDVdZt"
   },
   "source": [
    "-------------------------\n",
    "\n",
    "Add features and qualities needed:\n",
    "\n",
    "\n",
    "8) Secondary page for showing the different model outputs. (REMOVED)\n",
    "\n",
    "   * (The outputs can be displayed in a carousel manner that shows which model is used on top left.) (use st.tabs while doing this)\n",
    "\n",
    "9) Red background with black and white text for general Akbank asthetic. (Sidebar title and headers and the general page title should be turned to white.)\n",
    "\n",
    "10) Area for viewing general model information and if desired, tweaking the parameters.)\n",
    "  \n",
    "   * (The areas are done however, general information regarding the models should be filled for all models.)\n",
    "   \n",
    "-------------------------\n",
    "\n",
    "* On the computation side, look up: 384x384 generation to superresolution (DLSS) and neural style transfer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tLhIKgwvxCp9"
   },
   "source": [
    "Taskbar\n",
    "\n",
    "-------------------------\n",
    "POSSIBLE ADDITIONS FOR HIGHER QUALITY:\n",
    "\n",
    "1) Add scheduler selector:\n",
    "\n",
    "* Add K-LMS scheduler for SD1-4.\n",
    "\n",
    "* Add DPMSolverMultistep scheduler for SD2-1.\n",
    "\n",
    "2) Look up more parameters to further optimize stable diffusion.\n",
    "\n",
    "3) Add VQGAN + Clıp for variation:\n",
    "\n",
    "* Article: https://learn.adafruit.com/generating-ai-art-with-vqgan-clip/basic-use\n",
    "\n",
    "* Google Colab: https://colab.research.google.com/drive/1go6YwMFe5MX6XM9tv-cnQiSTU50N9EeT#scrollTo=VA1PHoJrRiK9\n",
    "\n",
    "4) Output three generated images (?)\n",
    "\n",
    "5) Instead of dreambooth try out LoRA for the fine-tuning for faster results and easier sharing, storage, and re-use. (LoRA is an improved finetuning method where instead of finetuning all the weights that constitute the weight matrix of the pre-trained large language model, two smaller matrices that approximate this larger matrix are fine-tuned.)\n",
    "\n",
    "-------------------------\n",
    "\n",
    "REMAINING:\n",
    "\n",
    "FlowBMs, implement their models, create and connect their parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r9N0upAsikms"
   },
   "source": [
    "IMPORTANT:\n",
    "\n",
    "There is a CUDA memory problem because we are trying to import too many things.\n",
    "\n",
    "Placeholder solution: Import modules when you need, within the functions that are called based on model. (Implemented)\n",
    "\n",
    "Better solution: Import modules like above but unless the next model is the same type that requires the same modules delete the modules within the functions. (Need to keep track of the previous model type)  \n",
    "\n",
    "! Could not implement the above, deletion can only occur within functions when the function is called."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nEw8fONYrsxR"
   },
   "source": [
    "GANS //\n",
    "\n",
    "\n",
    "Stuff to try out:\n",
    "\n",
    "* BigGAN: Is a large-scale GAN model developed by Google. It's designed for generating high-resolution images conditioned on text descriptions or class labels.\n",
    "\n",
    "* StyleGAN:  (Style Generative Adversarial Network) is a generative model architecture designed for generating high-resolution and highly realistic images, particularly faces and artworks.\n",
    "\n",
    " * StyleGAN2, which refines the architecture and training methods, and StyleGAN2-ADA (Adaptive Discriminator Augmentation), which further enhances training stability and image quality.\n",
    "\n",
    "* CLIP+GAN: Combining CLIP with GANs, you can condition the generation process on textual prompts and achieve text-to-image synthesis with improved context awareness.\n",
    "\n",
    "* AttnGAN: Attention Generative Adversarial Network (AttnGAN) is designed explicitly for fine-grained text-to-image generation. It uses attention mechanisms to focus on different parts of the text description while generating corresponding image details.\n",
    "\n",
    "* TAC-GAN: Text Conditioned Auxiliary Classifier Generative Adversarial Network, presents the study of generating realistic images through text-to-image synthesis with the help of GANs.\n",
    "\n",
    "-------------------------\n",
    "\n",
    "Possible GANs to implement: (Especially look at Stacked/Stack GANs if you have time.)\n",
    "\n",
    "* Stacked GAN: Stacked GAN refers to a general concept of stacking multiple GANs or GAN-like models on top of each other to improve the quality of generated images.\n",
    "\n",
    "  * StackGAN and StackGAN++: StackGAN is a particular implementation of stacked GANs designed for the task of text-to-image synthesis. It was developed to generate photo-realistic images from textual descriptions.\n",
    "\n",
    "  * Can use a StackGAN when you need to generate images from a completely different representation (e.g., from text-based descriptions).\n",
    "  *  authors repo: https://github.com/hanzhanggit/StackGAN\n",
    "\n",
    "* Information Maximizing GAN (Not verry useful for this specific task but can be fitted if needed.)\n",
    "\n",
    "* Super Resolution GAN:\n",
    "\n",
    "  *  Can use an SRGAN when you need to upscale images while recovering or preserving fine-grain, high-fidelity details. (Can be used to initially generate low dimensional images with other generational models, then be used to upscale them (?))\n",
    "\n",
    "-------------------------\n",
    "\n",
    "If everything else fails: pixelRNN, text-2-image, DiscoGAN, and IsGAN.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ASbquYicGser"
   },
   "source": [
    "FLOW BASED GENERATION MODELS //\n",
    "\n",
    "* Glow\n",
    "* RealNVP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qZlDXrkyMmJF"
   },
   "source": [
    "*** Dreambooth: Ideal parameters for cartoonish avatar generation: ***\n",
    "\n",
    "* for 8 training images\n",
    "\n",
    "  with_prior_preservation\n",
    "\n",
    "  train_batch_size=2 (effect not tested)\n",
    "\n",
    "  train_text_encoder\n",
    "\n",
    "  mixed_precision=\"fp16\"\n",
    "\n",
    "  use_8bit_adam\n",
    "\n",
    "  gradient_accumulation_steps=1\n",
    "\n",
    "  learning_rate=1e-6\n",
    "\n",
    "  num_class_images=100 (effect not tested)\n",
    "\n",
    "  sample_batch_size=4 (effect not tested)\n",
    "\n",
    "  max_train_steps=640 and 664\n",
    "  \n",
    "   * Previously tried 700 (and higher, untill 1200), presents a high risk of overfitting and every 1 image out of 5 is unusable. As of now the ideal number of training steps seems to be *** 80-83 epochs per image ***.\n",
    "\n",
    "  save_interval=10000\n",
    "\n",
    "  save_sample_prompt=\"yigitya person\"\n",
    "\n",
    "  ------------------------------------------------\n",
    "\n",
    "  prompt = \"A cartoonish avatar version of yigitya person.\"\n",
    "\n",
    "  negative_prompt = \"weird eyes, blurry eyes\"\n",
    "\n",
    "  num_samples = 1\n",
    "\n",
    "  guidance_scale = 9\n",
    "\n",
    "  num_inference_steps = 100\n",
    "\n",
    " ------------------------------------------------\n",
    "\n",
    " Accurate filenames were used, file names identical to instance prompt yigitya person. Also tried unrelated file names, ***does not seem to make a difference.***\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hXu5fkUyvTML"
   },
   "source": [
    "# IMPLEMENTATION:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sMiqDuguH_kR",
    "outputId": "20bd2275-a5c6-4d0c-ddf0-769df46f13a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: diffusers in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (0.15.0.dev0)\n",
      "Collecting diffusers\n",
      "  Obtaining dependency information for diffusers from https://files.pythonhosted.org/packages/2e/ed/58a13f88dfcdd1bcfeabf44d4c9861979551339348e579fd6559c64b12e0/diffusers-0.21.4-py3-none-any.whl.metadata\n",
      "  Using cached diffusers-0.21.4-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: transformers in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (4.33.3)\n",
      "Requirement already satisfied: scipy in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (1.11.3)\n",
      "Requirement already satisfied: Pillow in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from diffusers) (10.0.1)\n",
      "Requirement already satisfied: filelock in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from diffusers) (3.12.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.13.2 in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from diffusers) (0.17.3)\n",
      "Requirement already satisfied: importlib-metadata in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from diffusers) (6.8.0)\n",
      "Requirement already satisfied: numpy in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from diffusers) (1.26.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from diffusers) (2023.8.8)\n",
      "Requirement already satisfied: requests in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from diffusers) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from diffusers) (0.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from huggingface-hub>=0.13.2->diffusers) (2023.9.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from huggingface-hub>=0.13.2->diffusers) (4.7.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from importlib-metadata->diffusers) (3.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from requests->diffusers) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from requests->diffusers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from requests->diffusers) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from requests->diffusers) (2023.7.22)\n",
      "Using cached diffusers-0.21.4-py3-none-any.whl (1.5 MB)\n",
      "Installing collected packages: diffusers\n",
      "  Attempting uninstall: diffusers\n",
      "    Found existing installation: diffusers 0.15.0.dev0\n",
      "    Uninstalling diffusers-0.15.0.dev0:\n",
      "      Successfully uninstalled diffusers-0.15.0.dev0\n",
      "Successfully installed diffusers-0.21.4\n",
      "Requirement already satisfied: diffusers in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (0.21.4)\n",
      "Requirement already satisfied: transformers in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (4.33.3)\n",
      "Requirement already satisfied: accelerate in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (0.23.0)\n",
      "Requirement already satisfied: scipy in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (1.11.3)\n",
      "Requirement already satisfied: safetensors in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (0.3.3)\n",
      "Requirement already satisfied: Pillow in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from diffusers) (10.0.1)\n",
      "Requirement already satisfied: filelock in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from diffusers) (3.12.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.13.2 in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from diffusers) (0.17.3)\n",
      "Requirement already satisfied: importlib-metadata in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from diffusers) (6.8.0)\n",
      "Requirement already satisfied: numpy in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from diffusers) (1.26.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from diffusers) (2023.8.8)\n",
      "Requirement already satisfied: requests in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from diffusers) (2.31.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: psutil in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from accelerate) (2.0.1)\n",
      "Requirement already satisfied: fsspec in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from huggingface-hub>=0.13.2->diffusers) (2023.9.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from huggingface-hub>=0.13.2->diffusers) (4.7.1)\n",
      "Requirement already satisfied: sympy in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from importlib-metadata->diffusers) (3.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from requests->diffusers) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from requests->diffusers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from requests->diffusers) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from requests->diffusers) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "# For application and stable diffusion models:\n",
    "!pip install streamlit -q\n",
    "!pip install --upgrade diffusers transformers scipy\n",
    "!pip install diffusers transformers accelerate scipy safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "pj7hAEVLB4zr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'FuseDream' already exists and is not an empty directory.\n",
      "Requirement already satisfied: ftfy in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (6.1.1)\n",
      "Requirement already satisfied: regex in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (2023.8.8)\n",
      "Requirement already satisfied: tqdm in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (4.66.1)\n",
      "Requirement already satisfied: numpy in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (1.26.0)\n",
      "Requirement already satisfied: scipy in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (1.11.3)\n",
      "Requirement already satisfied: h5py in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (3.9.0)\n",
      "Requirement already satisfied: lpips==0.1.4 in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (0.1.4)\n",
      "Requirement already satisfied: torch>=0.4.0 in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from lpips==0.1.4) (2.0.1)\n",
      "Requirement already satisfied: torchvision>=0.2.1 in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from lpips==0.1.4) (0.15.2)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from ftfy) (0.2.5)\n",
      "Requirement already satisfied: filelock in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from torch>=0.4.0->lpips==0.1.4) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from torch>=0.4.0->lpips==0.1.4) (4.7.1)\n",
      "Requirement already satisfied: sympy in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from torch>=0.4.0->lpips==0.1.4) (1.12)\n",
      "Requirement already satisfied: networkx in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from torch>=0.4.0->lpips==0.1.4) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from torch>=0.4.0->lpips==0.1.4) (3.1.2)\n",
      "Requirement already satisfied: requests in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from torchvision>=0.2.1->lpips==0.1.4) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from torchvision>=0.2.1->lpips==0.1.4) (10.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from jinja2->torch>=0.4.0->lpips==0.1.4) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from requests->torchvision>=0.2.1->lpips==0.1.4) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from requests->torchvision>=0.2.1->lpips==0.1.4) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from requests->torchvision>=0.2.1->lpips==0.1.4) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from requests->torchvision>=0.2.1->lpips==0.1.4) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from sympy->torch>=0.4.0->lpips==0.1.4) (1.3.0)\n",
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to /private/var/folders/1r/dx6fjhj10k37w5dnl4mrmvhc0000gn/T/pip-req-build-xxeud83_\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /private/var/folders/1r/dx6fjhj10k37w5dnl4mrmvhc0000gn/T/pip-req-build-xxeud83_\n",
      "  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: ftfy in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from clip==1.0) (6.1.1)\n",
      "Requirement already satisfied: regex in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from clip==1.0) (2023.8.8)\n",
      "Requirement already satisfied: tqdm in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from clip==1.0) (4.66.1)\n",
      "Requirement already satisfied: torch in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from clip==1.0) (2.0.1)\n",
      "Requirement already satisfied: torchvision in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from clip==1.0) (0.15.2)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from ftfy->clip==1.0) (0.2.5)\n",
      "Requirement already satisfied: filelock in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from torch->clip==1.0) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from torch->clip==1.0) (4.7.1)\n",
      "Requirement already satisfied: sympy in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from torch->clip==1.0) (1.12)\n",
      "Requirement already satisfied: networkx in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from torch->clip==1.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from torch->clip==1.0) (3.1.2)\n",
      "Requirement already satisfied: numpy in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from torchvision->clip==1.0) (1.26.0)\n",
      "Requirement already satisfied: requests in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from torchvision->clip==1.0) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from torchvision->clip==1.0) (10.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from jinja2->torch->clip==1.0) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from requests->torchvision->clip==1.0) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from requests->torchvision->clip==1.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from requests->torchvision->clip==1.0) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from requests->torchvision->clip==1.0) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
      "Requirement already satisfied: gdown in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (4.7.1)\n",
      "Requirement already satisfied: filelock in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from gdown) (3.12.4)\n",
      "Requirement already satisfied: requests[socks] in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from gdown) (2.31.0)\n",
      "Requirement already satisfied: six in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from gdown) (1.16.0)\n",
      "Requirement already satisfied: tqdm in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from gdown) (4.66.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from gdown) (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from beautifulsoup4->gdown) (2.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from requests[socks]->gdown) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from requests[socks]->gdown) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from requests[socks]->gdown) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from requests[socks]->gdown) (2023.7.22)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages (from requests[socks]->gdown) (1.7.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (uriginal): https://drive.google.com/uc?id=1sOZ9og9kJLsqMNhaDnPJgzVsBZQ1sjZ5\n",
      "From (redirected): https://drive.google.com/uc?id=1sOZ9og9kJLsqMNhaDnPJgzVsBZQ1sjZ5&confirm=t&uuid=8b553a01-2e17-4f37-8339-98d5a9a0201e\n",
      "To: /Users/berkedenizbozyigit/Downloads/biggan-512.pth\n",
      "100%|████████████████████████████████████████| 330M/330M [00:32<00:00, 10.0MB/s]\n"
     ]
    }
   ],
   "source": [
    "# For GAN models:\n",
    "!git clone https://github.com/gnobitab/FuseDream.git\n",
    "!pip install ftfy regex tqdm numpy scipy h5py lpips==0.1.4\n",
    "!pip install git+https://github.com/openai/CLIP.git\n",
    "!pip install gdown\n",
    "!gdown 'https://drive.google.com/uc?id=1sOZ9og9kJLsqMNhaDnPJgzVsBZQ1sjZ5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "_PQxAx-n056w"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5x555555.JPG\n",
      "Antrag auf Immatrikulation.pdf\n",
      "\u001b[34mCATS_DOGS.zip.download\u001b[m\u001b[m\n",
      "\u001b[34mDoktar_Case_Study\u001b[m\u001b[m\n",
      "Doktar_Case_Study_Report-2.pdf\n",
      "Doktar_Case_Study_Report-3.pdf\n",
      "Doktar_Case_Study_Report.pdf\n",
      "\u001b[34mDoktar_Machine_Learning_Engineer_Case_Study\u001b[m\u001b[m\n",
      "Eczacibasi_Wed Aug 09 2023 10_32_04 GMT+0300 (GMT+03_00).pdf\n",
      "\u001b[34mFuseDream\u001b[m\u001b[m\n",
      "Image_Generation_UI_Project.ipynb\n",
      "\u001b[34mInstall Spotify.app\u001b[m\u001b[m\n",
      "Machine_Learning_Engineer_Case_Study.pdf\n",
      "Recomendation letter 2.pdf\n",
      "Recommendation letter.pdf\n",
      "\u001b[34mSublime Text.app\u001b[m\u001b[m\n",
      "\u001b[34mTF_2_Notebooks_and_Data\u001b[m\u001b[m\n",
      "\u001b[34mTF_2_Notebooks_and_Data-2\u001b[m\u001b[m\n",
      "Transcript.pdf\n",
      "UK (1).docx\n",
      "UK.docx\n",
      "\u001b[34mUNZIP_FOR_NOTEBOOKS_FINAL\u001b[m\u001b[m\n",
      "\u001b[34mUNZIP_FOR_NOTEBOOKS_FINAL-2\u001b[m\u001b[m\n",
      "University courses.pdf\n",
      "Unknown\n",
      "Unknown-2\n",
      "Unknown-3\n",
      "\u001b[34mVisual Studio Code.app\u001b[m\u001b[m\n",
      "WhatsApp Unknown 2023-09-17 at 20.03.23.zip\n",
      "\u001b[34mWhatsApp Unknown 2023-09-17 at 20.03.52\u001b[m\u001b[m\n",
      "WhatsApp Unknown 2023-09-17 at 20.03.52.zip\n",
      "YAPILMASI GEREKENLER.xlsx\n",
      "\u001b[34mYORK OFFER DOCS\u001b[m\u001b[m\n",
      "akbank.jpeg\n",
      "akbank.jpg\n",
      "biggan-512.pth\n",
      "doktor_case_study_new.py\n",
      "dvdrental.tar\n",
      "exercises.tar\n",
      "googlechrome.dmg\n",
      "kc_house_data.csv\n",
      "myspark.pem\n",
      "pgadmin4-7.5-x86_64.dmg\n",
      "postgresql-15.3-2-osx.dmg\n",
      "pycharm-community-2023.1.3.dmg\n",
      "requirements.txt\n",
      "sabancı.jpeg\n",
      "sabancı.jpg\n",
      "sabancı.png\n",
      "spiderman.jpg\n",
      "/Users/berkedenizbozyigit/Downloads/FuseDream\n"
     ]
    }
   ],
   "source": [
    "!ls\n",
    "!cp biggan-512.pth FuseDream/BigGAN_utils/weights/\n",
    "%cd FuseDream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SFu0lOjIGvI1",
    "outputId": "8328360e-5180-45a8-e111-b15aa698a9bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: wget\n",
      "zsh:1: command not found: wget\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement triton (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for triton\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[229 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m /Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages/torch/utils/cpp_extension.py:476: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
      "  \u001b[31m   \u001b[0m   warnings.warn(msg.format('we could not find ninja.'))\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m creating build\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers\n",
      "  \u001b[31m   \u001b[0m copying xformers/_deprecation_warning.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers\n",
      "  \u001b[31m   \u001b[0m copying xformers/version.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers\n",
      "  \u001b[31m   \u001b[0m copying xformers/checkpoint.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers\n",
      "  \u001b[31m   \u001b[0m copying xformers/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers\n",
      "  \u001b[31m   \u001b[0m copying xformers/test.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers\n",
      "  \u001b[31m   \u001b[0m copying xformers/utils.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers\n",
      "  \u001b[31m   \u001b[0m copying xformers/_cpp_lib.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers\n",
      "  \u001b[31m   \u001b[0m copying xformers/info.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/triton/fused_linear_layer.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/triton/sum_strided.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/triton/vararg_kernel.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/triton/k_activations.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/triton/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/triton/k_layer_norm.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/triton/k_sum.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/triton/utils.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/triton/k_fused_matmul_fw.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/triton/dropout.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/triton/k_dropout.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/triton/softmax.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/triton/layer_norm.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/triton/k_fused_matmul_bw.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/triton/k_softmax.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/triton\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/components\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/simplicial_embedding.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/residual.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/reversible.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/activations.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/multi_head_dispatch.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/input_projection.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/patch_embedding.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_mem_eff_attention.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_indexing.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_mlp.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_triton_stride_sum.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_blocksparse_transformers.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_mem_eff_attn_decoder.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_transformer.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_revnet.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_swiglu.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_triton_layernorm.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_causal_blocksparse.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_triton_fused_linear.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_triton_blocksparse.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_triton_softmax.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/utils.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_triton_dropout.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_nystrom_utils.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_attn_decoding.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_multi_head_dispatch.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_sddmm.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_core.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/rmsnorm.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/swiglu_op.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/unbind.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/rope_padded.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/common.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/indexing.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/profiler\n",
      "  \u001b[31m   \u001b[0m copying xformers/profiler/device_limits.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/profiler\n",
      "  \u001b[31m   \u001b[0m copying xformers/profiler/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/profiler\n",
      "  \u001b[31m   \u001b[0m copying xformers/profiler/api.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/profiler\n",
      "  \u001b[31m   \u001b[0m copying xformers/profiler/slow_ops_profiler.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/profiler\n",
      "  \u001b[31m   \u001b[0m copying xformers/profiler/profiler.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/profiler\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/sparse\n",
      "  \u001b[31m   \u001b[0m copying xformers/sparse/_csr_ops.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/sparse\n",
      "  \u001b[31m   \u001b[0m copying xformers/sparse/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/sparse\n",
      "  \u001b[31m   \u001b[0m copying xformers/sparse/utils.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/sparse\n",
      "  \u001b[31m   \u001b[0m copying xformers/sparse/blocksparse_tensor.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/sparse\n",
      "  \u001b[31m   \u001b[0m copying xformers/sparse/csr_tensor.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/sparse\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/helpers\n",
      "  \u001b[31m   \u001b[0m copying xformers/helpers/test_utils.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/helpers\n",
      "  \u001b[31m   \u001b[0m copying xformers/helpers/hierarchical_configs.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/helpers\n",
      "  \u001b[31m   \u001b[0m copying xformers/helpers/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/helpers\n",
      "  \u001b[31m   \u001b[0m copying xformers/helpers/timm_sparse_attention.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/helpers\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/fused_softmax.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/flash_blocksparse_attn_interface.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/flash_blocksparse_attention.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/bert_padding.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/flash_attn_triton_og.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/flash_attn_triton.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/flash_attn_interface.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/factory\n",
      "  \u001b[31m   \u001b[0m copying xformers/factory/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/factory\n",
      "  \u001b[31m   \u001b[0m copying xformers/factory/hydra_helper.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/factory\n",
      "  \u001b[31m   \u001b[0m copying xformers/factory/block_factory.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/factory\n",
      "  \u001b[31m   \u001b[0m copying xformers/factory/model_factory.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/factory\n",
      "  \u001b[31m   \u001b[0m copying xformers/factory/block_configs.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/factory\n",
      "  \u001b[31m   \u001b[0m copying xformers/factory/weight_init.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/factory\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/global_tokens.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/ortho.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/blocksparse.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/local.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/compositional.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/pooling.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/_sputnik_sparse.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/core.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/lambda_layer.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/random.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/fourier_mix.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/scaled_dot_product.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/utils.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/attention_mask.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/linformer.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/attention_patterns.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/visual.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/sparsity_config.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/nystrom.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/favor.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/base.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/components/feedforward\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/feedforward/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/feedforward\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/feedforward/mixture_of_experts.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/feedforward\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/feedforward/mlp.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/feedforward\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/feedforward/conv_mlp.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/feedforward\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/feedforward/fused_mlp.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/feedforward\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/feedforward/base.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/feedforward\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/components/positional_embedding\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/positional_embedding/vocab.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/positional_embedding\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/positional_embedding/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/positional_embedding\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/positional_embedding/param.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/positional_embedding\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/positional_embedding/sine.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/positional_embedding\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/positional_embedding/rotary.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/positional_embedding\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/positional_embedding/base.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/positional_embedding\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/components/attention/feature_maps\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/feature_maps/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/attention/feature_maps\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/feature_maps/softmax.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/attention/feature_maps\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/feature_maps/base.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/attention/feature_maps\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks/LRA\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/LRA/batch_submit.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks/LRA\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/LRA/batch_fetch_results.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks/LRA\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/LRA/run_with_submitit.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks/LRA\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/LRA/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks/LRA\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/LRA/run_tasks.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks/LRA\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/LRA/run_grid_search.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks/LRA\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks/LRA/code\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/LRA/code/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks/LRA/code\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/LRA/code/model_wrapper.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks/LRA/code\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/LRA/code/dataset.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks/LRA/code\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/triton/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/triton/rope_padded_kernels.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/triton/rmsnorm_kernels.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/triton\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/fmha\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/decoder.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/fmha\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/triton.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/fmha\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/dispatch.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/fmha\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/fmha\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/attn_bias.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/fmha\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/common.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/fmha\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/flash.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/fmha\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/small_k.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/fmha\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/cutlass.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/fmha\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/triton_splitk.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/fmha\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/losses\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/losses/cross_entropy.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/losses\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/losses/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/losses\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/layers\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/layers/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/layers\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/layers/patch_embed.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/layers\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/layers/rotary.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/layers\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/utils\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/utils/pretrained.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/utils\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/utils/generation.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/utils\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/utils/benchmark.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/utils\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/utils/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/utils\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/utils/distributed.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/utils\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/models/bigcode.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/models/gptj.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/models/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/models/opt.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/models/llama.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/models/vit.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/models/baichuan.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/models/bert.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/models/falcon.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/models/gpt_neox.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/models/gpt.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/models\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/ops/activations.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/ops/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/ops/fused_dense.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/ops/rms_norm.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/ops/layer_norm.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/ops\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/modules\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/modules/embedding.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/modules\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/modules/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/modules\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/modules/mlp.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/modules\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/modules/block.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/modules\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/modules/mha.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/modules\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/ops/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/ops/triton/cross_entropy.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/ops/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/ops/triton/linear.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/ops/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/ops/triton/k_activations.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/ops/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/ops/triton/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/ops/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/ops/triton/mlp.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/ops/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/ops/triton/rotary.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/ops/triton\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m building 'xformers._C' extension\n",
      "  \u001b[31m   \u001b[0m creating build/temp.macosx-11.1-arm64-cpython-311\n",
      "  \u001b[31m   \u001b[0m creating build/temp.macosx-11.1-arm64-cpython-311/xformers\n",
      "  \u001b[31m   \u001b[0m creating build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc\n",
      "  \u001b[31m   \u001b[0m creating build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc/attention\n",
      "  \u001b[31m   \u001b[0m creating build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc/attention/autograd\n",
      "  \u001b[31m   \u001b[0m creating build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc/attention/cpu\n",
      "  \u001b[31m   \u001b[0m creating build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc/indexing\n",
      "  \u001b[31m   \u001b[0m creating build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc/swiglu\n",
      "  \u001b[31m   \u001b[0m clang -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /Applications/anaconda3/envs/Avatar/include -arch arm64 -fPIC -O2 -isystem /Applications/anaconda3/envs/Avatar/include -arch arm64 -I/private/var/folders/1r/dx6fjhj10k37w5dnl4mrmvhc0000gn/T/pip-install-7l2zed5c/xformers_06ae08a88c7f4b898b4fba195ebcdfb4/xformers/csrc -I/Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages/torch/include -I/Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages/torch/include/TH -I/Applications/anaconda3/envs/Avatar/lib/python3.11/site-packages/torch/include/THC -I/Applications/anaconda3/envs/Avatar/include/python3.11 -c xformers/csrc/attention/attention.cpp -o build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc/attention/attention.o -O3 -std=c++17 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_clang\\\" -DPYBIND11_STDLIB=\\\"_libcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1002\\\" -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\n",
      "  \u001b[31m   \u001b[0m You have not agreed to the Xcode license agreements. Please run 'sudo xcodebuild -license' from within a Terminal window to review and agree to the Xcode and Apple SDKs license.\n",
      "  \u001b[31m   \u001b[0m error: command '/usr/bin/clang' failed with exit code 69\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for xformers\u001b[0m\u001b[31m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[31mERROR: Could not build wheels for xformers, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Dreambooth requirements:\n",
    "!wget -q https://github.com/ShivamShrirao/diffusers/raw/main/examples/dreambooth/train_dreambooth.py\n",
    "!wget -q https://github.com/ShivamShrirao/diffusers/raw/main/scripts/convert_diffusers_to_original_stable_diffusion.py\n",
    "%pip install -qq git+https://github.com/ShivamShrirao/diffusers\n",
    "%pip install -q -U --pre triton                                                 # use either this or the one at the bottom\n",
    "%pip install -q accelerate transformers ftfy bitsandbytes==0.35.0 gradio natsort safetensors xformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "naBgaWjoGxzP",
    "outputId": "c1daf9d5-4ea7-4cbd-d17f-ed0895a5f1a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accelerate configuration saved at /Users/berkedenizbozyigit/.cache/huggingface/accelerate/default_config.yaml\r\n"
     ]
    }
   ],
   "source": [
    "!accelerate config default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "moUs9zRIBa0z",
    "outputId": "a41795e4-efe0-4950-bc5a-3d962d45d7d6"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1228265611.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[7], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    writefile akbankavatargenerator.py\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# V3: (Final Version)  -->  (Fix issue: Additional fine-tuning information within the app has red underline when deployed.)\n",
    "\n",
    "writefile akbankavatargenerator.py\n",
    "#####################################\n",
    "import streamlit as st\n",
    "import base64\n",
    "import torch\n",
    "from PIL import Image\n",
    "#####################################\n",
    "# Functions:\n",
    "\n",
    "# Function for calling any of the two stable diffusion models, imports and deletes required modules, parameters are self explanatory, returns the image:\n",
    "def stablediffusioncall(option, user_prompt, user_n_prompt, i_inference_steps, i_initial_seed, i_guidance_scale, situation):\n",
    "  from diffusers import StableDiffusionPipeline\n",
    "\n",
    "  if option == \"Stable Diffusion v1-4\":        # Uses a PNDM scheduler\n",
    "    model_id = \"CompVis/stable-diffusion-v1-4\"\n",
    "  elif option == \"Stable Diffusion 2-1\":\n",
    "    model_id = \"stabilityai/stable-diffusion-2-1\" # Uses a DDIM scheduler\n",
    "\n",
    "  device = \"cuda\"\n",
    "  pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "  pipe = pipe.to(device)\n",
    "\n",
    "  prompt = user_prompt\n",
    "  negative_prompt = user_n_prompt\n",
    "\n",
    "  if situation:     # If custom parameters are chosen, parameters are given the user input values. If not, the pipeline is called without change.\n",
    "    num_inference_steps = i_inference_steps\n",
    "    guidance_scale = i_guidance_scale\n",
    "\n",
    "    if i_initial_seed is not None:        # In case of wanting to use custom seed, use the user input value. If not the pipeline is called without seed input.\n",
    "      generator = torch.Generator(\"cuda\").manual_seed(i_initial_seed)\n",
    "      image = pipe(prompt=prompt, num_inference_steps=num_inference_steps, guidance_scale=guidance_scale, negative_prompt=negative_prompt, generator=generator ).images[0]\n",
    "    else:\n",
    "      image = pipe(prompt=prompt, num_inference_steps=num_inference_steps, guidance_scale=guidance_scale, negative_prompt=negative_prompt).images[0]\n",
    "  else:\n",
    "    image = pipe(prompt=prompt, negative_prompt=negative_prompt).images[0]\n",
    "\n",
    "  del StableDiffusionPipeline\n",
    "  return image\n",
    "\n",
    "# Function for calling FuseDream model, imports and deletes required modules, parameters are self explanatory, returns the image:\n",
    "def fusedreamcall(user_prompt, INIT_ITERS, OPT_ITERS, NUM_BASIS, SEED, use_seed):\n",
    "  from tqdm import tqdm\n",
    "  from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "  import torchvision\n",
    "  import BigGAN_utils.utils as utils\n",
    "  import clip\n",
    "  import torch.nn.functional as F\n",
    "  from DiffAugment_pytorch import DiffAugment\n",
    "  import numpy as np\n",
    "  from fusedream_utils import FuseDreamBaseGenerator, get_G\n",
    "  import sys\n",
    "\n",
    "  sentence = user_prompt\n",
    "\n",
    "  if use_seed:           # If the function is called using default parameter values SEED and use_seed are given the value None. If the utils.seed_rng(SEED) is called with None it gives an error, therefore the if statment is used.\n",
    "    utils.seed_rng(SEED)\n",
    "\n",
    "  sys.argv = ['']\n",
    "\n",
    "  G, config = get_G(512)\n",
    "  generator = FuseDreamBaseGenerator(G, config, 10)\n",
    "  z_cllt, y_cllt = generator.generate_basis(sentence, init_iters=INIT_ITERS, num_basis=NUM_BASIS)\n",
    "\n",
    "  z_cllt_save = torch.cat(z_cllt).cpu().numpy()\n",
    "  y_cllt_save = torch.cat(y_cllt).cpu().numpy()\n",
    "  img, z, y = generator.optimize_clip_score(z_cllt, y_cllt, sentence, latent_noise=False, augment=True, opt_iters=OPT_ITERS, optimize_y=True)\n",
    "\n",
    "  image = img\n",
    "  image = (image / 2 + 0.5).clamp(0, 1)\n",
    "  image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "  images = (image * 255).round().astype(\"uint8\")\n",
    "  pil_images = [Image.fromarray(image) for image in images]\n",
    "  image = pil_images[0]\n",
    "\n",
    "  del tqdm\n",
    "  del torchvision\n",
    "  del utils\n",
    "  del clip\n",
    "  del F\n",
    "  del DiffAugment\n",
    "  del FuseDreamBaseGenerator\n",
    "  del get_G\n",
    "  return image\n",
    "\n",
    "# Function for fine-tuning stable diffusion v1-5 model, imports and deletes required modules, parameters are self explanatory, returns the fine-tuned model pipeline:\n",
    "def finetunesdcall(u_token, u_class, plant, t_b_size, l_r, n_c_images, s_b_size, m_t_steps, uploaded_files):\n",
    "  # Settings (including model selection):\n",
    "\n",
    "  import subprocess\n",
    "\n",
    "  s_prompt=u_token+\" \"+u_class\n",
    "\n",
    "  MODEL_NAME = \"runwayml/stable-diffusion-v1-5\"\n",
    "\n",
    "  OUTPUT_DIR = \"/content/stable_diffusion_weights/\" + u_token\n",
    "\n",
    "  try:\n",
    "      # Construct the shell command\n",
    "      command = f\"mkdir -p {OUTPUT_DIR}\"\n",
    "      # Execute the shell command\n",
    "      subprocess.run(command, shell=True, check=True)\n",
    "  except subprocess.CalledProcessError as e:\n",
    "      # Handle any errors that occur during command execution\n",
    "      print(f\"Error: {e}\")\n",
    "\n",
    "  # Choosing the instance and class prompts:\n",
    "\n",
    "  # You can also add multiple concepts here, try tweaking `--max_train_steps` accordingly.\n",
    "  # `class_data_dir` contains the regularization images\n",
    "  concepts_list = [\n",
    "      {\n",
    "          \"instance_prompt\":      f\"photo of {u_token} man\",\n",
    "          \"class_prompt\":         \"photo of a man\",\n",
    "          \"instance_data_dir\":    \"/content/data/\" + u_token,\n",
    "          \"class_data_dir\":       \"/content/data/man\"\n",
    "      },\n",
    "      {\n",
    "          \"instance_prompt\":      f\"photo of {u_token} woman\",\n",
    "          \"class_prompt\":         \"photo of a woman\",\n",
    "          \"instance_data_dir\":    \"/content/data/\" + u_token,\n",
    "          \"class_data_dir\":       \"/content/data/woman\"\n",
    "      },\n",
    "      {\n",
    "          \"instance_prompt\":      f\"photo of {u_token} person\",\n",
    "          \"class_prompt\":         \"photo of a person\",\n",
    "          \"instance_data_dir\":    \"/content/data/\" + u_token,\n",
    "          \"class_data_dir\":       \"/content/data/person\"\n",
    "      }\n",
    "  ]\n",
    "\n",
    "  import json\n",
    "  import os\n",
    "  for c in concepts_list:\n",
    "      os.makedirs(c[\"instance_data_dir\"], exist_ok=True)\n",
    "\n",
    "  keep_cl = []\n",
    "  for c in concepts_list:\n",
    "    a_class = c['instance_prompt'].split()\n",
    "    if a_class[-1] == u_class:\n",
    "      keep_cl.append(c)\n",
    "\n",
    "  with open(\"concepts_list.json\", \"w\") as f:\n",
    "      json.dump(keep_cl, f, indent=4)\n",
    "\n",
    "  # Upload your images\n",
    "\n",
    "  from google.colab import files\n",
    "  import shutil\n",
    "\n",
    "  for c in concepts_list:\n",
    "    a_class = c['instance_prompt'].split()\n",
    "    if a_class[-1] == u_class:\n",
    "      uploaded = uploaded_files\n",
    "      for element in uploaded:\n",
    "        filename = element.name\n",
    "        dst_path = os.path.join(c['instance_data_dir'], filename)\n",
    "        with open(dst_path, \"wb\") as f:\n",
    "            f.write(element.read())\n",
    "\n",
    "  # Fine-tuning the SD model:\n",
    "\n",
    "  # Tweak the parameters for desired image quality --> 1200 training steps = 40 newly introdued images for 30 epochs.\n",
    "  command = f\"\"\"\n",
    "          python3 train_dreambooth.py \\\n",
    "          --pretrained_model_name_or_path={MODEL_NAME} \\\n",
    "          --pretrained_vae_name_or_path=\"stabilityai/sd-vae-ft-mse\" \\\n",
    "          --output_dir={OUTPUT_DIR} \\\n",
    "          --revision=\"fp16\" \\\n",
    "          --with_prior_preservation --prior_loss_weight=1.0 \\\n",
    "          --seed={plant} \\\n",
    "          --resolution=512 \\\n",
    "          --train_batch_size={t_b_size} \\\n",
    "          --train_text_encoder \\\n",
    "          --mixed_precision=\"fp16\" \\\n",
    "          --use_8bit_adam \\\n",
    "          --gradient_accumulation_steps=1 \\\n",
    "          --learning_rate={l_r} \\\n",
    "          --lr_scheduler=\"constant\" \\\n",
    "          --lr_warmup_steps=0 \\\n",
    "          --num_class_images={n_c_images} \\\n",
    "          --sample_batch_size={s_b_size} \\\n",
    "          --max_train_steps={m_t_steps} \\\n",
    "          --save_interval=10000 \\\n",
    "          --save_sample_prompt=\"{s_prompt}\" \\\n",
    "          --concepts_list=\"concepts_list.json\"\n",
    "      \"\"\"\n",
    "  try:\n",
    "      # Execute the shell command\n",
    "      subprocess.run(command, shell=True, check=True, executable=\"/bin/bash\")\n",
    "  except subprocess.CalledProcessError as e:\n",
    "      # Handle any errors that occur during command execution\n",
    "      print(f\"Error: {e}\")\n",
    "\n",
    "  # Reduce the `--save_interval` to lower than `--max_train_steps` to save weights from intermediate steps.\n",
    "  # `--save_sample_prompt` can be same as `--instance_prompt` to generate intermediate samples (saved along with weights in samples)\n",
    "\n",
    "  # Specify the weights directory to use (leave blank for latest):\n",
    "\n",
    "  from natsort import natsorted\n",
    "  from glob import glob\n",
    "  WEIGHTS_DIR = natsorted(glob(OUTPUT_DIR + os.sep + \"*\"))[-1]\n",
    "\n",
    "  # Inference:\n",
    "\n",
    "  import torch\n",
    "  from torch import autocast\n",
    "  from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
    "  from IPython.display import display\n",
    "\n",
    "  model_path = WEIGHTS_DIR\n",
    "\n",
    "  pipe = StableDiffusionPipeline.from_pretrained(model_path, safety_checker=None, torch_dtype=torch.float16).to(\"cuda\")\n",
    "  pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n",
    "  pipe.enable_xformers_memory_efficient_attention()\n",
    "  g_cuda = None\n",
    "\n",
    "  del json\n",
    "  del os\n",
    "  del files\n",
    "  del shutil\n",
    "  del natsorted\n",
    "  del glob\n",
    "  del torch\n",
    "  del autocast\n",
    "  del display\n",
    "  del subprocess\n",
    "  return pipe\n",
    "\n",
    "# Function for generating prompt conditioned images with fine-tuned stable diffusion v1-5 model, imports and deletes required modules, parameters are self explanatory, returns the image:\n",
    "def dbgeneratecall(pipe, user_prompt, user_n_prompt, guidance_scale, num_inference_steps, use_seed, s33d):\n",
    "\n",
    "  import torch\n",
    "  from torch import autocast\n",
    "  from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
    "\n",
    "  # Run for generating images.\n",
    "\n",
    "  prompt = user_prompt\n",
    "  negative_prompt = user_n_prompt\n",
    "  num_samples = 1\n",
    "  height = 512\n",
    "  width = 512\n",
    "\n",
    "  if use_seed:\n",
    "    g_cuda = torch.Generator(device='cuda')\n",
    "    seed = s33d\n",
    "    g_cuda.manual_seed(seed)\n",
    "    with autocast(\"cuda\"), torch.inference_mode():\n",
    "        images = pipe(\n",
    "            prompt,\n",
    "            height=height,\n",
    "            width=width,\n",
    "            negative_prompt=negative_prompt,\n",
    "            num_images_per_prompt=num_samples,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            guidance_scale=guidance_scale,\n",
    "            generator=g_cuda\n",
    "        ).images\n",
    "  else:\n",
    "    with autocast(\"cuda\"), torch.inference_mode():\n",
    "        images = pipe(\n",
    "            prompt,\n",
    "            height=height,\n",
    "            width=width,\n",
    "            negative_prompt=negative_prompt,\n",
    "            num_images_per_prompt=num_samples,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            guidance_scale=guidance_scale,\n",
    "        ).images\n",
    "\n",
    "  for img in images:\n",
    "    generated = img\n",
    "\n",
    "  del torch\n",
    "  del autocast\n",
    "  return generated\n",
    "\n",
    "###############################################################################################################\n",
    "\n",
    "def main():\n",
    "\n",
    "  icon = Image.open('/content/icon.png')\n",
    "  st.set_page_config(page_icon = icon, layout=\"wide\")\n",
    "\n",
    "  #background_image = Image.open('/content/background.jpg')\n",
    "\n",
    "  col1, coli1, col2, coli2, col3, = st.columns((1,0.25,2,0.25,1))\n",
    "\n",
    "  image = Image.open('/content/akbank.jpg')\n",
    "  image2 = Image.open('/content/sabancı.png')\n",
    "\n",
    "  if 'files_uploaded' not in st.session_state:  # Keeps track of if any image has been uploaded (keeps boolean value). Initializing it beforehand, prevents being called before being initialized error\n",
    "    st.session_state.files_uploaded = False\n",
    "\n",
    "  if 'pictures' not in st.session_state:  # Keeps track of if any image has been uploaded (keeps the pictures themselves). Initializing it beforehand, prevents being called before being initialized error\n",
    "    st.session_state.pictures = False\n",
    "\n",
    "  if 'pipe_s' not in st.session_state:  # Keeps track of if pipe_s has been returned by finetunesdcall (keeps the pipeline). Initializing it beforehand, prevents being called before being initialized error\n",
    "    st.session_state.pipe_s = False\n",
    "\n",
    "  if 'fine_tuned' not in st.session_state:  # Keeps track of if the model has been fine-tuned. Initializing it beforehand, prevents being called before being initialized error\n",
    "    st.session_state.fine_tuned = False\n",
    "\n",
    "  with col1:\n",
    "    st.image(image, caption='', width = 500, use_column_width=True)\n",
    "    with st.sidebar:\n",
    "      st.title(':red[MODELS]')\n",
    "      st.header('General model information', divider='red')\n",
    "\n",
    "      with st.expander(\"Stable Diffusion v1-4\"):\n",
    "        st.write(\"PLaceholder.\")\n",
    "\n",
    "      with st.expander(\"Stable Diffusion 2-1\"):\n",
    "        st.write(\"PLaceholder.\")\n",
    "\n",
    "      with st.expander(\"FuseDream (CLIP+BigGAN)\"):\n",
    "        st.write(\"PLaceholder.\")\n",
    "\n",
    "      with st.expander(\"Non-specified Flow based model\"):\n",
    "        st.write(\"PLaceholder.\")\n",
    "\n",
    "      st.header('Settings to tweak the models', divider='red')\n",
    "      # situation = True --> Want to use custom parameters, situation = False --> Want to use default parameters\n",
    "      situation = st.toggle('Use custom parameters for models')\n",
    "      if situation:\n",
    "        with st.expander('Diffusion model parameters:'):  # Have 3 parameters\n",
    "\n",
    "          i_inference_steps = st.number_input('Number of inference steps', step = 1)\n",
    "\n",
    "          use_seed = st.toggle('Enter a seed value', help = 'If seed value not entered, each generated image will be given a random one.')\n",
    "          if use_seed:\n",
    "            i_initial_seed = st.number_input('Initial seed', value = 0, step = 1)\n",
    "          else:\n",
    "            i_initial_seed = None\n",
    "\n",
    "          i_guidance_scale = st.slider('Guidance scale', 1.0, 25.0, 7.5, step = 0.5)\n",
    "\n",
    "        with st.expander('FuseDream (CLIP+BigGAN) parameters:'): # Has 5 parameters (including the boolean use_seed)\n",
    "\n",
    "          INIT_ITERS = st.number_input('Number of images used for initialization', step = 1)\n",
    "\n",
    "          OPT_ITERS = st.number_input('Number of iterations', step = 1)\n",
    "\n",
    "          NUM_BASIS = st.number_input('Number of basis images', step = 1)\n",
    "\n",
    "          use_seed = st.toggle('Enter a seed value', key = 4, help = 'If seed value not entered, each generated image will be given a random one.')\n",
    "          if use_seed:\n",
    "            SEED = st.number_input('Initial seed', value = 0, step = 1, key = 5)\n",
    "          else:\n",
    "            SEED = None\n",
    "\n",
    "        with st.expander('Non-specified Flow based model parameters:'): # Not yet implemented, has 0 parameters\n",
    "          st.write(\"PLaceholder.\")\n",
    "\n",
    "      st.header(' ', divider='red')\n",
    "      # Dreambooth is separated from the situation block since it should not be called with default parameters as the parameters need to be entered according to the external data and its use. (TLDR Attune parameters per each use for good results.)\n",
    "      with st.expander('Dreambooth parameters:'): # Has 8 parameters for fine-tuning and 4 parameters for image generation (including use_seed boolean)\n",
    "\n",
    "        u_token = st.text_input('Unique token', '', placeholder='Enter a unique token.', help = 'example: name_surname or zwx')\n",
    "\n",
    "        u_class = st.text_input('Class of reg images', '', placeholder='Enter a regularization image set class.', help = 'Available class names: man, woman, person')\n",
    "\n",
    "        plant = st.number_input('Initial seed', value = 0, step = 1, key = 3)\n",
    "\n",
    "        t_b_size = st.number_input('Training bach size', step =1, key = 1)\n",
    "\n",
    "        l_r = st.slider('Learning rate', 1, 6, step = 1)\n",
    "        l_r = l_r * 1e-6\n",
    "\n",
    "        n_c_images = st.number_input('Number of class images', step =1)\n",
    "\n",
    "        s_b_size = st.number_input('Sample batch size', step =1)\n",
    "\n",
    "        m_t_steps = st.number_input('Training steps', step =1)\n",
    "\n",
    "        ft_start = st.button('Start fine-tuning')\n",
    "\n",
    "        if ft_start and st.session_state.files_uploaded:   # Checks if any photos have been uploaded before starting fine-tuning, if not gives an error\n",
    "          process = st.empty()            # Creating a Fine-tuning...\n",
    "          process.text('Fine-tuning...')\n",
    "\n",
    "          st.session_state.pipe_s = finetunesdcall(u_token, u_class, plant, t_b_size, l_r, n_c_images, s_b_size, m_t_steps, st.session_state.pictures)              # Calls dreambooth fine-tuning function\n",
    "          st.session_state.fine_tuned = True\n",
    "\n",
    "          process.empty()        # Deletes Fine-tuning... text\n",
    "          st.write('Fine-tuning completed, now an image can be generated')\n",
    "        elif ft_start and not st.session_state.files_uploaded:\n",
    "          st.error('Please upload images to fine-tune the model', icon=\"🚨\")\n",
    "\n",
    "        guidance_scale = st.slider('Guidance scale', 1.0, 25.0, 7.5, step = 0.5, key = 12)\n",
    "\n",
    "        num_inference_steps = st.number_input('Number of inference steps', step = 1, key = 13)\n",
    "\n",
    "        use_seed = st.toggle('Enter a seed value', key = 14, help = 'If seed value not entered, each generated image will be given a random one.')\n",
    "        if use_seed:\n",
    "          s33d = st.number_input('Initial seed', value = 0, step = 1, key = 5)\n",
    "        else:\n",
    "          s33d = None\n",
    "\n",
    "  with col2:\n",
    "    st.title(\"Akbank Avatar Generator\")\n",
    "    use_db = st.toggle('Use Dreambooth')  # If you want to use Dreambooth, reveals the following untill the line:\n",
    "    if use_db:\n",
    "      st.write('Show us what you look like 😊')\n",
    "      uploaded_files = st.file_uploader(\"Upload your photos here...\", type=['png', 'jpg', 'jpeg'], accept_multiple_files=True, help = 'Please only upload JPEGs or PNGs of 512x512 pixels.')\n",
    "      if uploaded_files:\n",
    "        st.session_state.pictures = uploaded_files\n",
    "        st.session_state.files_uploaded = True\n",
    "      information = st.checkbox('See additional information for fine tuned image generation with Dreambooth')\n",
    "      if information:               # Additional infromation toggle\n",
    "        display_text = \"\"\"\n",
    "        Ideal parameters for cartoonish avatar generation:\n",
    "\n",
    "        (Used 8 inputted training images)\n",
    "        ▪️ Training batch size = 2 (effect not tested)\n",
    "        ▪️ Learning rate = 1e-6\n",
    "        ▪️ Number of class images = 100 (effect not tested)\n",
    "        ▪️ Sample batch size = 4 (effect not tested)\n",
    "        ▪️ Training steps = 640 and 664\n",
    "          (The ideal number of training steps seems to be 80-83 epochs per image.)\n",
    "        ▪️ Sample prompt = unique token + class of regularization images\n",
    "        ▪️ Prompt = A cartoonish avatar version of Sample prompt.\n",
    "        ▪️ Negative prompt = Up to the user\n",
    "        ▪️ Guidance scale = 9\n",
    "        ▪️ Number of inference steps = 100\n",
    "        \"\"\"\n",
    "        st.text_area(\"Ideal parameters for different prompts and uses:\", display_text)\n",
    "\n",
    "      st.write(':red[Attention:] Dreambooth is currently only available for Stable-Diffusion-v1-5. When using Dreambooth this model is automatically implemented, no need to choose models 😅')\n",
    "################################################################################\n",
    "    option = st.selectbox('What model would you like to use?', ('Stable Diffusion v1-4', 'Stable Diffusion 2-1', 'FuseDream (CLIP+BigGAN)', 'Non-specified Flow based model'), placeholder = \"Select your model...\")\n",
    "\n",
    "    user_prompt = st.text_input('Prompt', '', placeholder='* Required to fill this area.')\n",
    "\n",
    "    user_n_prompt = st.text_input('Negative prompt (Optional)', '')\n",
    "    st.write(':red[Attention:] Negative prompts are currently only available for diffusion models')\n",
    "\n",
    "    generation_start = st.button('Start generating')\n",
    "    if generation_start and user_prompt == '':          # Ensures a prompt is entered\n",
    "      st.error('Please enter a prompt', icon=\"🚨\")\n",
    "    elif generation_start and user_prompt != '':\n",
    "\n",
    "      select_sd = option == 'Stable Diffusion v1-4' or option == 'Stable Diffusion 2-1'\n",
    "\n",
    "      process = st.empty()            # Creating a Generating...\n",
    "      process.text('Generating...')\n",
    "\n",
    "      if use_db:        # Selected model is Dreambooth fine tuning on Stable Diffusion v1-5\n",
    "        if st.session_state.fine_tuned:\n",
    "          generated_image = dbgeneratecall(st.session_state.pipe_s, user_prompt, user_n_prompt, guidance_scale, num_inference_steps, use_seed, s33d)\n",
    "\n",
    "          process.empty()            # Deletes Generating... text\n",
    "          st.write('')\n",
    "\n",
    "          st.image(generated_image, caption= 'Generated image using Dreambooth fine-tuning.')       # Display the generated image with caption\n",
    "        else:\n",
    "          process.empty()            # Deletes Generating... text\n",
    "          st.error('Please first fine-tune the model', icon=\"🚨\")\n",
    "\n",
    "      elif select_sd:       # Selected model is a Stable Diffusion one\n",
    "        if not situation:\n",
    "          i_inference_steps = 0         # If chosen to use default parameters, gives placeholder values that will be changed to default values within the stablediffusioncall function\n",
    "          i_guidance_scale = 0\n",
    "          i_initial_seed = 0\n",
    "\n",
    "        generated_image = stablediffusioncall(option, user_prompt, user_n_prompt, i_inference_steps, i_initial_seed ,i_guidance_scale, situation)       # Calls stablediffusioncall function\n",
    "\n",
    "        process.empty()            # Deletes Generating... text\n",
    "        st.write('')\n",
    "\n",
    "        st.image(generated_image, caption= f'Generated image using {option}.')       # Display the generated image with caption\n",
    "\n",
    "      elif option == 'FuseDream (CLIP+BigGAN)':    # Selected model is FuseDream\n",
    "        if not situation:\n",
    "          INIT_ITERS =  1000\n",
    "          OPT_ITERS = 1000                 # If chosen to use default parameters, gives default values\n",
    "          NUM_BASIS = 10\n",
    "          SEED = None\n",
    "          use_seed = False\n",
    "\n",
    "        generated_image = fusedreamcall(user_prompt, INIT_ITERS, OPT_ITERS, NUM_BASIS, SEED, use_seed)            # Calls fusedreamcall function\n",
    "\n",
    "        process.empty()           # Deletes Generating... text\n",
    "        st.write('')\n",
    "\n",
    "        st.image(generated_image, caption= f'Generated image using {option}.')    # Display the generated image with caption\n",
    "\n",
    "      elif option == 'Non-specified Flow based model':      # Selected model is a Non-specified Flow based model\n",
    "        process.empty()                  # Deletes Generating... text\n",
    "        st.write('')\n",
    "\n",
    "        st.write('Error: This model is not implemented yet', key = 51)\n",
    "\n",
    "  with col3:\n",
    "    st.image(image2, caption='', width = 150, use_column_width=True, output_format=\"PNG\")\n",
    "\n",
    "###############################################################################################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LLMwq1e-KcYY",
    "outputId": "89de27cd-6c7c-49bc-8833-a5892c017e19"
   },
   "outputs": [],
   "source": [
    "!curl ipecho.net/plain   # -----> Enter with remote IP address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BYr8hRh4JCLT"
   },
   "outputs": [],
   "source": [
    "!streamlit run akbankavatargenerator.py & npx localtunnel --port 8501"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_cPOrXbFkvsd"
   },
   "source": [
    "OLD VERSIONS:\n",
    "-------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mxMpchvrIhlT"
   },
   "outputs": [],
   "source": [
    "# V1:\n",
    "\n",
    "%%writefile akbankavatargenerator.py\n",
    "#####################################\n",
    "import streamlit as st\n",
    "import base64\n",
    "import torch\n",
    "from PIL import Image\n",
    "#####################################\n",
    "# Functions:\n",
    "\n",
    "def stablediffusioncall(option, user_prompt, user_n_prompt, i_inference_steps, i_initial_seed, i_guidance_scale, situation):\n",
    "  from diffusers import StableDiffusionPipeline\n",
    "\n",
    "  if option == \"Stable Diffusion v1-4\":        # Uses PNDM scheduler\n",
    "    model_id = \"CompVis/stable-diffusion-v1-4\"\n",
    "  elif option == \"Stable Diffusion 2-1\":\n",
    "    model_id = \"stabilityai/stable-diffusion-2-1\" # Uses DDIM scheduler\n",
    "\n",
    "  device = \"cuda\"\n",
    "  pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "  pipe = pipe.to(device)\n",
    "\n",
    "  prompt = user_prompt\n",
    "  negative_prompt = user_n_prompt\n",
    "\n",
    "  if situation:\n",
    "    num_inference_steps = i_inference_steps\n",
    "    guidance_scale = i_guidance_scale\n",
    "\n",
    "    if i_initial_seed is not None:\n",
    "      generator = torch.Generator(\"cuda\").manual_seed(i_initial_seed)\n",
    "      image = pipe(prompt=prompt, num_inference_steps=num_inference_steps, guidance_scale=guidance_scale, negative_prompt=negative_prompt, generator=generator ).images[0]\n",
    "    else:\n",
    "      image = pipe(prompt=prompt, num_inference_steps=num_inference_steps, guidance_scale=guidance_scale, negative_prompt=negative_prompt).images[0]\n",
    "  else:\n",
    "    image = pipe(prompt=prompt, negative_prompt=negative_prompt).images[0]\n",
    "\n",
    "  del StableDiffusionPipeline\n",
    "  return image\n",
    "\n",
    "def fusedreamcall(user_prompt, INIT_ITERS, OPT_ITERS, NUM_BASIS, SEED, use_seed):\n",
    "  from tqdm import tqdm\n",
    "  from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "  import torchvision\n",
    "  import BigGAN_utils.utils as utils\n",
    "  import clip\n",
    "  import torch.nn.functional as F\n",
    "  from DiffAugment_pytorch import DiffAugment\n",
    "  import numpy as np\n",
    "  from fusedream_utils import FuseDreamBaseGenerator, get_G\n",
    "  import sys\n",
    "\n",
    "  sentence = user_prompt\n",
    "\n",
    "  if use_seed:\n",
    "    utils.seed_rng(SEED)\n",
    "\n",
    "  sys.argv = ['']\n",
    "\n",
    "  G, config = get_G(512)\n",
    "  generator = FuseDreamBaseGenerator(G, config, 10)\n",
    "  z_cllt, y_cllt = generator.generate_basis(sentence, init_iters=INIT_ITERS, num_basis=NUM_BASIS)\n",
    "\n",
    "  z_cllt_save = torch.cat(z_cllt).cpu().numpy()\n",
    "  y_cllt_save = torch.cat(y_cllt).cpu().numpy()\n",
    "  img, z, y = generator.optimize_clip_score(z_cllt, y_cllt, sentence, latent_noise=False, augment=True, opt_iters=OPT_ITERS, optimize_y=True)\n",
    "  ### Set latent_noise = True yields slightly higher AugCLIP score, but slightly lower image quality.\n",
    "\n",
    "  image = img\n",
    "  image = (image / 2 + 0.5).clamp(0, 1)\n",
    "  image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "  images = (image * 255).round().astype(\"uint8\")\n",
    "  pil_images = [Image.fromarray(image) for image in images]\n",
    "  image = pil_images[0]\n",
    "\n",
    "  del tqdm\n",
    "  del torchvision\n",
    "  del utils\n",
    "  del clip\n",
    "  del F\n",
    "  del DiffAugment\n",
    "  del FuseDreamBaseGenerator\n",
    "  del get_G\n",
    "  return image\n",
    "\n",
    "#####################################\n",
    "\n",
    "def main():\n",
    "  #icon = Image.open('/content/icon.png')                                       # Remove comment\n",
    "  st.set_page_config(layout=\"wide\")                                             # Add page_icon = icon within the (), before layout\n",
    "\n",
    "  #background_image = Image.open('/content/background.jpg')                     # Remove comment\n",
    "\n",
    "\n",
    "  ##################################### For red background (Does not work):\n",
    "  page_bg_img = '''\n",
    "  <style>\n",
    "  body {\n",
    "  background-image: background_image\n",
    "  background-size: cover;\n",
    "  }\n",
    "  </style>\n",
    "  '''\n",
    "  st.markdown(page_bg_img, unsafe_allow_html=True)\n",
    "  #####################################\n",
    "\n",
    "  col1, coli1, col2, coli2, col3, = st.columns((1,0.25,2,0.25,1))\n",
    "\n",
    "  #image = Image.open('/content/akbank.jpg')                                    # Remove comment\n",
    "  #image2 = Image.open('/content/sabancı.png')                                  # Remove comment\n",
    "\n",
    "  with col1:\n",
    "    #st.image(image, caption='', width = 500, use_column_width=True)            # Remove comment\n",
    "    with st.sidebar:\n",
    "      st.title(':red[MODELS]')\n",
    "      st.header('General model information', divider='red')\n",
    "\n",
    "      with st.expander(\"Stable Diffusion v1-4\"):\n",
    "        st.write(\"PLaceholder.\")\n",
    "\n",
    "      with st.expander(\"Stable Diffusion 2-1\"):\n",
    "        st.write(\"PLaceholder.\")\n",
    "\n",
    "      with st.expander(\"FuseDream (CLIP+BigGAN)\"):\n",
    "        st.write(\"PLaceholder.\")\n",
    "\n",
    "      with st.expander(\"Non-specified Flow based model\"):\n",
    "        st.write(\"PLaceholder.\")\n",
    "\n",
    "      st.header('Settings to tweak the models', divider='red')\n",
    "\n",
    "      situation = st.toggle('Use custom parameters for models')\n",
    "      if situation:\n",
    "        with st.expander('Diffusion model parameters:'):\n",
    "\n",
    "          i_inference_steps = st.number_input('Number of inference steps', step = 1)\n",
    "\n",
    "          use_seed = st.toggle('Enter a seed value', help = 'If seed value not entered, each generated image will be given a random one.')\n",
    "          if use_seed:\n",
    "            i_initial_seed = st.number_input('Initial seed', value = 0, step = 1)\n",
    "          else:\n",
    "            i_initial_seed = None\n",
    "\n",
    "          i_guidance_scale = st.slider('Guidance scale', 1.0, 50.0, 7.5, step = 0.5)\n",
    "\n",
    "        with st.expander('Dreambooth parameters:'):\n",
    "\n",
    "          id_inference_steps = st.number_input('Number of inference steps', step =1, key = 1)\n",
    "\n",
    "          use_seed = st.toggle('Enter a seed value', key = 2, help = 'If seed value not entered, each generated image will be given a random one.')\n",
    "          if use_seed:\n",
    "            id_initial_seed = st.number_input('Initial seed', value = 0, step = 1, key = 3)\n",
    "          else:\n",
    "            id_initial_seed = None\n",
    "\n",
    "          id_learning_rate = st.slider('Learning rate', 1, 6, step = 1)\n",
    "          id_learning_rate = id_learning_rate * 1e-6\n",
    "\n",
    "          id_training_steps = st.number_input('Training steps', step =1)\n",
    "\n",
    "          id_save_interval = st.number_input('Save interval', step =1, help = 'Should be lower than the number of training steps!')\n",
    "\n",
    "          id_train_batch_size = st.number_input('Training batch size', step =1, help = 'Should be at least 1 and at most the number of images you have uploaded!')\n",
    "\n",
    "          id_num_class_images = st.number_input('Number of classification images', step =1)\n",
    "\n",
    "        with st.expander('FuseDream (CLIP+BigGAN):'):\n",
    "\n",
    "          INIT_ITERS = st.number_input('Number of images used for initialization', step = 1)\n",
    "\n",
    "          OPT_ITERS = st.number_input('Number of iterations', step = 1)\n",
    "\n",
    "          NUM_BASIS = st.number_input('Number of basis images', step = 1)\n",
    "\n",
    "          use_seed = st.toggle('Enter a seed value', key = 4, help = 'If seed value not entered, each generated image will be given a random one.')\n",
    "          if use_seed:\n",
    "            SEED = st.number_input('Initial seed', value = 0, step = 1, key = 5)\n",
    "          else:\n",
    "            SEED = None\n",
    "\n",
    "        with st.expander('Non-specified Flow based model parameters:'):\n",
    "          st.write(\"PLaceholder.\")\n",
    "\n",
    "  with col2:\n",
    "    st.title(\"Akbank Avatar Generator\")\n",
    "\n",
    "    st.write('Show us what you look like 😊')\n",
    "    uploaded_file = st.file_uploader(\"Upload your photos here...\", type=['png', 'jpg', 'jpeg'], accept_multiple_files=True, help = 'Please only upload JPEGs or PNGs of 512x512 pixels.')\n",
    "    st.write('Attention: Fine tuning with external data is currently only available for diffusion models 😅')\n",
    "\n",
    "    option = st.selectbox('What model would you like to use?', ('Stable Diffusion v1-4', 'Stable Diffusion 2-1', 'FuseDream (CLIP+BigGAN)', 'Non-specified Flow based model'), placeholder = \"Select your model...\")\n",
    "\n",
    "    user_prompt = st.text_input('Prompt', '', placeholder='* Required to fill this area.')\n",
    "\n",
    "    user_n_prompt = st.text_input('Negative prompt (Optional)', '')\n",
    "    st.write('Attention: Negative prompts are currently only available for diffusion models')\n",
    "\n",
    "    generation_start = st.button('Start generating')\n",
    "    if generation_start and user_prompt == '':\n",
    "      st.error('Please enter a prompt', icon=\"🚨\")\n",
    "    elif generation_start and user_prompt != '':\n",
    "      process = st.empty()\n",
    "      process.text('Generating...')\n",
    "\n",
    "      if option == 'Stable Diffusion v1-4' or option == 'Stable Diffusion 2-1':\n",
    "        if not situation:\n",
    "          i_inference_steps = 0\n",
    "          i_guidance_scale = 0\n",
    "          i_initial_seed = 0\n",
    "\n",
    "        generated_image = stablediffusioncall(option, user_prompt, user_n_prompt, i_inference_steps, i_initial_seed ,i_guidance_scale, situation)\n",
    "\n",
    "        process.empty()            # Until the other models are implemented keep these 3 rows here. After, place them just before with col3, lined up to the beggining of the above elif.\n",
    "        st.write('')\n",
    "\n",
    "        st.image(generated_image, caption= f'Generated image using {option}.')\n",
    "\n",
    "      elif option == 'FuseDream (CLIP+BigGAN)':\n",
    "        if not situation:\n",
    "          INIT_ITERS =  1000\n",
    "          OPT_ITERS = 1000\n",
    "          NUM_BASIS = 10\n",
    "          SEED = None\n",
    "          use_seed = False\n",
    "\n",
    "        generated_image = fusedreamcall(user_prompt, INIT_ITERS, OPT_ITERS, NUM_BASIS, SEED, use_seed)\n",
    "\n",
    "        process.empty()            # Until the other models are implemented keep these 3 rows here. After, place them just before with col3, lined up to the beggining of the above elif.\n",
    "        st.write('')\n",
    "\n",
    "        st.image(generated_image, caption= f'Generated image using {option}.')\n",
    "\n",
    "      elif option == 'Non-specified Flow based model':\n",
    "        st.write('This model is not implemented yet', key = 51)\n",
    "\n",
    "  with col3:\n",
    "    for i in range(60):\n",
    "      st.write('')\n",
    "    #st.image(image2, caption='', width = 150, use_column_width=False, output_format=\"PNG\")              # Remove comment\n",
    "\n",
    "#####################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GWkZM_DkW4we"
   },
   "outputs": [],
   "source": [
    "# V2: (Shelved)\n",
    "\n",
    "%%writefile akbankavatargenerator.py\n",
    "#####################################\n",
    "import streamlit as st\n",
    "import base64\n",
    "import torch\n",
    "from PIL import Image\n",
    "#####################################\n",
    "# Functions:\n",
    "\n",
    "# Function for calling any of the two stable diffusion models, imports and deletes required modules, parameters are self explanatory, returns the image:\n",
    "def stablediffusioncall(option, user_prompt, user_n_prompt, i_inference_steps, i_initial_seed, i_guidance_scale, situation):\n",
    "  from diffusers import StableDiffusionPipeline\n",
    "\n",
    "  if option == \"Stable Diffusion v1-4\":        # Uses a PNDM scheduler\n",
    "    model_id = \"CompVis/stable-diffusion-v1-4\"\n",
    "  elif option == \"Stable Diffusion 2-1\":\n",
    "    model_id = \"stabilityai/stable-diffusion-2-1\" # Uses a DDIM scheduler\n",
    "\n",
    "  device = \"cuda\"\n",
    "  pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "  pipe = pipe.to(device)\n",
    "\n",
    "  prompt = user_prompt\n",
    "  negative_prompt = user_n_prompt\n",
    "\n",
    "  if situation:     # If custom parameters are chosen, parameters are given the user input values. If not, the pipeline is called without change.\n",
    "    num_inference_steps = i_inference_steps\n",
    "    guidance_scale = i_guidance_scale\n",
    "\n",
    "    if i_initial_seed is not None:        # In case of wanting to use custom seed, use the user input value. If not the pipeline is called without seed input.\n",
    "      generator = torch.Generator(\"cuda\").manual_seed(i_initial_seed)\n",
    "      image = pipe(prompt=prompt, num_inference_steps=num_inference_steps, guidance_scale=guidance_scale, negative_prompt=negative_prompt, generator=generator ).images[0]\n",
    "    else:\n",
    "      image = pipe(prompt=prompt, num_inference_steps=num_inference_steps, guidance_scale=guidance_scale, negative_prompt=negative_prompt).images[0]\n",
    "  else:\n",
    "    image = pipe(prompt=prompt, negative_prompt=negative_prompt).images[0]\n",
    "\n",
    "  del StableDiffusionPipeline\n",
    "  return image\n",
    "\n",
    "# Function for calling FuseDream model, imports and deletes required modules, parameters are self explanatory returns the image:\n",
    "def fusedreamcall(user_prompt, INIT_ITERS, OPT_ITERS, NUM_BASIS, SEED, use_seed):\n",
    "  from tqdm import tqdm\n",
    "  from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "  import torchvision\n",
    "  import BigGAN_utils.utils as utils\n",
    "  import clip\n",
    "  import torch.nn.functional as F\n",
    "  from DiffAugment_pytorch import DiffAugment\n",
    "  import numpy as np\n",
    "  from fusedream_utils import FuseDreamBaseGenerator, get_G\n",
    "  import sys\n",
    "\n",
    "  sentence = user_prompt\n",
    "\n",
    "  if use_seed:           # If the function is called using default parameter values SEED and use_seed are given the value None. If the utils.seed_rng(SEED) is called with None it gives an error, therefore the if statment is used.\n",
    "    utils.seed_rng(SEED)\n",
    "\n",
    "  sys.argv = ['']\n",
    "\n",
    "  G, config = get_G(512)\n",
    "  generator = FuseDreamBaseGenerator(G, config, 10)\n",
    "  z_cllt, y_cllt = generator.generate_basis(sentence, init_iters=INIT_ITERS, num_basis=NUM_BASIS)\n",
    "\n",
    "  z_cllt_save = torch.cat(z_cllt).cpu().numpy()\n",
    "  y_cllt_save = torch.cat(y_cllt).cpu().numpy()\n",
    "  img, z, y = generator.optimize_clip_score(z_cllt, y_cllt, sentence, latent_noise=False, augment=True, opt_iters=OPT_ITERS, optimize_y=True)\n",
    "\n",
    "  image = img\n",
    "  image = (image / 2 + 0.5).clamp(0, 1)\n",
    "  image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "  images = (image * 255).round().astype(\"uint8\")\n",
    "  pil_images = [Image.fromarray(image) for image in images]\n",
    "  image = pil_images[0]\n",
    "\n",
    "  del tqdm\n",
    "  del torchvision\n",
    "  del utils\n",
    "  del clip\n",
    "  del F\n",
    "  del DiffAugment\n",
    "  del FuseDreamBaseGenerator\n",
    "  del get_G\n",
    "  return image\n",
    "\n",
    "###############################################################################################################\n",
    "\n",
    "def main():\n",
    "  icon = Image.open('/content/icon.png')\n",
    "  st.set_page_config(page_icon = icon, layout=\"wide\")\n",
    "\n",
    "  #background_image = Image.open('/content/background.jpg')\n",
    "\n",
    "  col1, coli1, col2, coli2, col3, = st.columns((1,0.25,2,0.25,1))\n",
    "\n",
    "  image = Image.open('/content/akbank.jpg')\n",
    "  image2 = Image.open('/content/sabancı.png')\n",
    "\n",
    "  with col1:\n",
    "    st.image(image, caption='', width = 500, use_column_width=True)\n",
    "    with st.sidebar:\n",
    "      st.title(':red[MODELS]')\n",
    "      st.header('General model information', divider='red')\n",
    "\n",
    "      with st.expander(\"Stable Diffusion v1-4\"):\n",
    "        st.write(\"PLaceholder.\")\n",
    "\n",
    "      with st.expander(\"Stable Diffusion 2-1\"):\n",
    "        st.write(\"PLaceholder.\")\n",
    "\n",
    "      with st.expander(\"FuseDream (CLIP+BigGAN)\"):\n",
    "        st.write(\"PLaceholder.\")\n",
    "\n",
    "      with st.expander(\"Non-specified Flow based model\"):\n",
    "        st.write(\"PLaceholder.\")\n",
    "\n",
    "      st.header('Settings to tweak the models', divider='red')\n",
    "      # situation = True --> Want to use custom parameters, situation = False --> Want to use default parameters\n",
    "      situation = st.toggle('Use custom parameters for models')\n",
    "      if situation:\n",
    "        with st.expander('Diffusion model parameters:'):  # Have 3 parameters\n",
    "\n",
    "          i_inference_steps = st.number_input('Number of inference steps', step = 1)\n",
    "\n",
    "          use_seed = st.toggle('Enter a seed value', help = 'If seed value not entered, each generated image will be given a random one.')\n",
    "          if use_seed:\n",
    "            i_initial_seed = st.number_input('Initial seed', value = 0, step = 1)\n",
    "          else:\n",
    "            i_initial_seed = None\n",
    "\n",
    "          i_guidance_scale = st.slider('Guidance scale', 1.0, 50.0, 7.5, step = 0.5)\n",
    "\n",
    "        with st.expander('FuseDream (CLIP+BigGAN) parameters:'): # Has 5 parameters (including the boolean use_seed)\n",
    "\n",
    "          INIT_ITERS = st.number_input('Number of images used for initialization', step = 1)\n",
    "\n",
    "          OPT_ITERS = st.number_input('Number of iterations', step = 1)\n",
    "\n",
    "          NUM_BASIS = st.number_input('Number of basis images', step = 1)\n",
    "\n",
    "          use_seed = st.toggle('Enter a seed value', key = 4, help = 'If seed value not entered, each generated image will be given a random one.')\n",
    "          if use_seed:\n",
    "            SEED = st.number_input('Initial seed', value = 0, step = 1, key = 5)\n",
    "          else:\n",
    "            SEED = None\n",
    "\n",
    "        with st.expander('Non-specified Flow based model parameters:'): # Not yet implemented, has 0 parameters\n",
    "          st.write(\"PLaceholder.\")\n",
    "\n",
    "      st.header(' ', divider='red')\n",
    "\n",
    "      with st.expander('Dreambooth parameters:'): # Has 9 parameters, Dreambooth is separated from the situation block since it should not be called with default parameters as the parameters need to be entered according to the external data and its use. (TLDR Attune parameters per each use for good results.)\n",
    "\n",
    "        u_token = st.text_input('Unique token', '', placeholder='Enter a unique token.', help = 'example: name_surname or zwx')\n",
    "\n",
    "        u_class = st.text_input('Class of reg images', '', placeholder='Enter a regularization image set class.', help = 'if restricted amount of available image sets, example: list of the available class names; else example: dog or person')\n",
    "\n",
    "        id_inference_steps = st.number_input('Number of inference steps', step =1, key = 1)\n",
    "\n",
    "        use_seed = st.toggle('Enter a seed value', key = 2, help = 'If seed value not entered, each generated image will be given a random one.')\n",
    "        if use_seed:\n",
    "          id_initial_seed = st.number_input('Initial seed', value = 0, step = 1, key = 3)\n",
    "        else:\n",
    "          id_initial_seed = None\n",
    "\n",
    "        id_learning_rate = st.slider('Learning rate', 1, 6, step = 1)\n",
    "        id_learning_rate = id_learning_rate * 1e-6\n",
    "\n",
    "        id_training_steps = st.number_input('Training steps', step =1)\n",
    "\n",
    "        id_save_interval = st.number_input('Save interval', step =1, help = 'Should be lower than the number of training steps!')\n",
    "\n",
    "        id_train_batch_size = st.number_input('Training batch size', step =1, help = 'Should be at least 1 and at most the number of images you have uploaded!')\n",
    "\n",
    "        id_num_class_images = st.number_input('Number of classification images', step =1)\n",
    "\n",
    "  with col2:\n",
    "    st.title(\"Akbank Avatar Generator\")\n",
    "    use_db = st.toggle('Use Dreambooth')  # If you want to use Dreambooth, reveals the following untill the line:\n",
    "    if use_db:\n",
    "      st.write('Show us what you look like 😊')\n",
    "      uploaded_file = st.file_uploader(\"Upload your photos here...\", type=['png', 'jpg', 'jpeg'], accept_multiple_files=True, help = 'Please only upload JPEGs or PNGs of 512x512 pixels.')\n",
    "      information = st.checkbox('See additional information for fine tuned image generation')\n",
    "      if information:               # Additional infromation toggle\n",
    "        st.write('PLaceholder')\n",
    "      st.write('Attention: Fine tuning with external data is currently only available for diffusion models 😅')\n",
    "################################################################################\n",
    "    option = st.selectbox('What model would you like to use?', ('Stable Diffusion v1-4', 'Stable Diffusion 2-1', 'FuseDream (CLIP+BigGAN)', 'Non-specified Flow based model'), placeholder = \"Select your model...\")\n",
    "\n",
    "    user_prompt = st.text_input('Prompt', '', placeholder='* Required to fill this area.')\n",
    "\n",
    "    user_n_prompt = st.text_input('Negative prompt (Optional)', '')\n",
    "    st.write('Attention: Negative prompts are currently only available for diffusion models')\n",
    "\n",
    "    generation_start = st.button('Start generating')\n",
    "    if generation_start and user_prompt == '':          # Ensures a prompt is entered\n",
    "      st.error('Please enter a prompt', icon=\"🚨\")\n",
    "    elif generation_start and user_prompt != '':\n",
    "\n",
    "      select_sd = option == 'Stable Diffusion v1-4' or option == 'Stable Diffusion 2-1'\n",
    "\n",
    "      if use_db and not select_sd:                                     # Ensures that a diffusion model is selected in the case of wanting to use Dreambooth\n",
    "        st.error('Please select a diffusion model', icon=\"🚨\")\n",
    "      else:\n",
    "\n",
    "        process = st.empty()            # Creating a Generating...\n",
    "        process.text('Generating...')\n",
    "\n",
    "        if use_db and select_sd:     # Selected model is a Stable Diffusion one with Dreambooth\n",
    "          process.empty()            # Deletes Generating... text\n",
    "          st.write('')\n",
    "\n",
    "          st.write('Error: This model is not implemented yet', key = 51)\n",
    "\n",
    "        elif select_sd:       # Selected model is a Stable Diffusion one\n",
    "          if not situation:\n",
    "            i_inference_steps = 0         # If chosen to use default parameters, gives placeholder values that will be changed to default values within the stablediffusioncall function\n",
    "            i_guidance_scale = 0\n",
    "            i_initial_seed = 0\n",
    "\n",
    "          generated_image = stablediffusioncall(option, user_prompt, user_n_prompt, i_inference_steps, i_initial_seed ,i_guidance_scale, situation)       # Calls stablediffusioncall function\n",
    "\n",
    "          process.empty()            # Deletes Generating... text\n",
    "          st.write('')\n",
    "\n",
    "          st.image(generated_image, caption= f'Generated image using {option}.')       # Display the generated image with caption\n",
    "\n",
    "        elif option == 'FuseDream (CLIP+BigGAN)':    # Selected model is FuseDream\n",
    "          if not situation:\n",
    "            INIT_ITERS =  1000\n",
    "            OPT_ITERS = 1000                 # If chosen to use default parameters, gives default values\n",
    "            NUM_BASIS = 10\n",
    "            SEED = None\n",
    "            use_seed = False\n",
    "\n",
    "          generated_image = fusedreamcall(user_prompt, INIT_ITERS, OPT_ITERS, NUM_BASIS, SEED, use_seed)            # Calls fusedreamcall function\n",
    "\n",
    "          process.empty()           # Deletes Generating... text\n",
    "          st.write('')\n",
    "\n",
    "          st.image(generated_image, caption= f'Generated image using {option}.')    # Display the generated image with caption\n",
    "\n",
    "        elif option == 'Non-specified Flow based model':      # Selected model is a Non-specified Flow based model\n",
    "          process.empty()                  # Deletes Generating... text\n",
    "          st.write('')\n",
    "\n",
    "          st.write('Error: This model is not implemented yet', key = 51)\n",
    "\n",
    "  with col3:\n",
    "    st.image(image2, caption='', width = 150, use_column_width=True, output_format=\"PNG\")\n",
    "\n",
    "###############################################################################################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_0OUZEbgG180"
   },
   "source": [
    "SANDBOX:\n",
    "---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q2qnGj3Olj24"
   },
   "outputs": [],
   "source": [
    "# Dreambooth requirements:\n",
    "!wget -q https://github.com/ShivamShrirao/diffusers/raw/main/examples/dreambooth/train_dreambooth.py\n",
    "!wget -q https://github.com/ShivamShrirao/diffusers/raw/main/scripts/convert_diffusers_to_original_stable_diffusion.py\n",
    "%pip install -qq git+https://github.com/ShivamShrirao/diffusers\n",
    "%pip install -q -U --pre triton                                                 # use either this or the one at the bottom\n",
    "%pip install -q accelerate transformers ftfy bitsandbytes==0.35.0 gradio natsort safetensors xformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CKJMf-kSlpZl",
    "outputId": "5b682d69-00b6-458f-fb2a-f0b3aec44a34"
   },
   "outputs": [],
   "source": [
    "!accelerate config default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mgtQGTYnG_Aj",
    "outputId": "26e84572-0c23-4843-996c-3384dbf6400e"
   },
   "outputs": [],
   "source": [
    "# Settings (including model selection):\n",
    "\n",
    "u_token=\"yigitya\"\n",
    "\n",
    "u_class=\"person\"\n",
    "\n",
    "plant=1337\n",
    "\n",
    "t_b_size=2\n",
    "\n",
    "l_r=1e-6\n",
    "\n",
    "n_c_images=100\n",
    "\n",
    "s_b_size=4\n",
    "\n",
    "m_t_steps=640\n",
    "\n",
    "s_prompt=u_token+\" \"+u_class\n",
    "\n",
    "MODEL_NAME = \"runwayml/stable-diffusion-v1-5\"\n",
    "\n",
    "OUTPUT_DIR = \"/content/stable_diffusion_weights/\" + u_token\n",
    "\n",
    "print(f\"[*] Weights will be saved at {OUTPUT_DIR}\")\n",
    "\n",
    "!mkdir -p $OUTPUT_DIR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f31noCw2JIgk"
   },
   "outputs": [],
   "source": [
    "# Choosing the instance and class prompts:\n",
    "\n",
    "# You can also add multiple concepts here, try tweaking `--max_train_steps` accordingly.\n",
    "# `class_data_dir` contains the regularization images\n",
    "concepts_list = [\n",
    "    {\n",
    "        \"instance_prompt\":      f\"photo of {u_token} man\",\n",
    "        \"class_prompt\":         \"photo of a man\",\n",
    "        \"instance_data_dir\":    \"/content/data/\" + u_token,\n",
    "        \"class_data_dir\":       \"/content/data/man\"\n",
    "    },\n",
    "    {\n",
    "        \"instance_prompt\":      f\"photo of {u_token} woman\",\n",
    "        \"class_prompt\":         \"photo of a woman\",\n",
    "        \"instance_data_dir\":    \"/content/data/\" + u_token,\n",
    "        \"class_data_dir\":       \"/content/data/woman\"\n",
    "    },\n",
    "    {\n",
    "        \"instance_prompt\":      f\"photo of {u_token} person\",\n",
    "        \"class_prompt\":         \"photo of a person\",\n",
    "        \"instance_data_dir\":    \"/content/data/\" + u_token,\n",
    "        \"class_data_dir\":       \"/content/data/person\"\n",
    "    }\n",
    "]\n",
    "\n",
    "import json\n",
    "import os\n",
    "for c in concepts_list:\n",
    "    os.makedirs(c[\"instance_data_dir\"], exist_ok=True)\n",
    "\n",
    "keep_cl = []\n",
    "for c in concepts_list:\n",
    "  a_class = c['instance_prompt'].split()\n",
    "  if a_class[-1] == u_class:\n",
    "    keep_cl.append(c)\n",
    "\n",
    "with open(\"concepts_list.json\", \"w\") as f:\n",
    "    json.dump(keep_cl, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-mp0hHitPKqf"
   },
   "outputs": [],
   "source": [
    "# Upload your images by running this cell:\n",
    "\n",
    "import os\n",
    "from google.colab import files\n",
    "import shutil\n",
    "\n",
    "for c in concepts_list:\n",
    "  a_class = c['instance_prompt'].split()\n",
    "  if a_class[-1] == u_class:\n",
    "    print(f\"Uploading instance images for `{c['instance_prompt']}`\")\n",
    "    uploaded = files.upload()\n",
    "    for filename in uploaded.keys():\n",
    "        dst_path = os.path.join(c['instance_data_dir'], filename)\n",
    "        shutil.move(filename, dst_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EuNAt6TNUYBs"
   },
   "outputs": [],
   "source": [
    "# Fine-tuning the SD model:\n",
    "\n",
    "# Tweak the parameters for desired image quality --> 1200 training steps = 40 newly introdued images for 30 epochs.\n",
    "!python3 train_dreambooth.py \\\n",
    "  --pretrained_model_name_or_path=$MODEL_NAME \\\n",
    "  --pretrained_vae_name_or_path=\"stabilityai/sd-vae-ft-mse\" \\\n",
    "  --output_dir=$OUTPUT_DIR \\\n",
    "  --revision=\"fp16\" \\\n",
    "  --with_prior_preservation --prior_loss_weight=1.0 \\\n",
    "  --seed=$plant \\\n",
    "  --resolution=512 \\\n",
    "  --train_batch_size=$t_b_size \\\n",
    "  --train_text_encoder \\\n",
    "  --mixed_precision=\"fp16\" \\\n",
    "  --use_8bit_adam \\\n",
    "  --gradient_accumulation_steps=1 \\\n",
    "  --learning_rate=$l_r \\\n",
    "  --lr_scheduler=\"constant\" \\\n",
    "  --lr_warmup_steps=0 \\\n",
    "  --num_class_images=$n_c_images \\\n",
    "  --sample_batch_size=$s_b_size \\\n",
    "  --max_train_steps=$m_t_steps \\\n",
    "  --save_interval=10000 \\\n",
    "  --save_sample_prompt=\"$s_prompt\" \\\n",
    "  --concepts_list=\"concepts_list.json\"\n",
    "\n",
    "# Reduce the `--save_interval` to lower than `--max_train_steps` to save weights from intermediate steps.\n",
    "# `--save_sample_prompt` can be same as `--instance_prompt` to generate intermediate samples (saved along with weights in samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qlEiJBZmXl0G",
    "outputId": "0220d78e-d6cc-4ed8-f9ce-7f6ce358e8fa"
   },
   "outputs": [],
   "source": [
    "# Specify the weights directory to use (leave blank for latest):\n",
    "\n",
    "from natsort import natsorted\n",
    "from glob import glob\n",
    "import os\n",
    "WEIGHTS_DIR = natsorted(glob(OUTPUT_DIR + os.sep + \"*\"))[-1]\n",
    "print(f\"[*] WEIGHTS_DIR={WEIGHTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oRHh1T5CYThG"
   },
   "outputs": [],
   "source": [
    "# Inference:\n",
    "\n",
    "import torch\n",
    "from torch import autocast\n",
    "from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
    "from IPython.display import display\n",
    "\n",
    "model_path = WEIGHTS_DIR\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_path, safety_checker=None, torch_dtype=torch.float16).to(\"cuda\")\n",
    "pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n",
    "pipe.enable_xformers_memory_efficient_attention()\n",
    "g_cuda = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1x-IZ6yYYeCQ",
    "outputId": "d7fb5e35-dcaf-4e5c-8ecb-30522718a6b6"
   },
   "outputs": [],
   "source": [
    "# Can set a random seed here for reproducibility:\n",
    "\n",
    "g_cuda = torch.Generator(device='cuda')\n",
    "seed = 52362\n",
    "g_cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561,
     "referenced_widgets": [
      "cfdb349137e14d3da6ee62fe30ffb3bc",
      "1a8af1e2cf7b4bdc83b5ebd0bccdaf58",
      "ea8aee72bab8429ab1801efc416ac974",
      "75dafec73f5143b998475f33d1ae0c42",
      "d8579797fa02430f9d483e66294853e3",
      "77d8da83f47b486d8133419bd9e50541",
      "e3a3780722854663bc2efb382bacba8c",
      "f1e45bee064b4b56989b73428196eb83",
      "82ce1ce4042646a688c26139f2e8b726",
      "ed0a082cfb25476ca1ce7e2d97280113",
      "248d075f097d4c41b01e5dc5bc1633af"
     ]
    },
    "id": "eApBIYYqhjxs",
    "outputId": "5e652800-8331-484d-c7e6-d28acf5d89d1"
   },
   "outputs": [],
   "source": [
    "# Run for generating images.\n",
    "\n",
    "prompt = \"A version of yigitya person as an cosmonaut with the helmet and suit in outher space.\"\n",
    "negative_prompt = \"weird eyes, blurry eyes, fat \"\n",
    "num_samples = 1\n",
    "guidance_scale = 9\n",
    "num_inference_steps = 100\n",
    "height = 512\n",
    "width = 512\n",
    "\n",
    "with autocast(\"cuda\"), torch.inference_mode():\n",
    "    images = pipe(\n",
    "        prompt,\n",
    "        height=height,\n",
    "        width=width,\n",
    "        negative_prompt=negative_prompt,\n",
    "        num_images_per_prompt=num_samples,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        guidance_scale=guidance_scale,\n",
    "        generator=g_cuda\n",
    "    ).images\n",
    "\n",
    "for img in images:\n",
    "    display(img)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "MP5fXsaWu5gs",
    "_cPOrXbFkvsd",
    "_0OUZEbgG180"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1a8af1e2cf7b4bdc83b5ebd0bccdaf58": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_77d8da83f47b486d8133419bd9e50541",
      "placeholder": "​",
      "style": "IPY_MODEL_e3a3780722854663bc2efb382bacba8c",
      "value": "100%"
     }
    },
    "248d075f097d4c41b01e5dc5bc1633af": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "75dafec73f5143b998475f33d1ae0c42": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ed0a082cfb25476ca1ce7e2d97280113",
      "placeholder": "​",
      "style": "IPY_MODEL_248d075f097d4c41b01e5dc5bc1633af",
      "value": " 100/100 [00:16&lt;00:00,  5.93it/s]"
     }
    },
    "77d8da83f47b486d8133419bd9e50541": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "82ce1ce4042646a688c26139f2e8b726": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "cfdb349137e14d3da6ee62fe30ffb3bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1a8af1e2cf7b4bdc83b5ebd0bccdaf58",
       "IPY_MODEL_ea8aee72bab8429ab1801efc416ac974",
       "IPY_MODEL_75dafec73f5143b998475f33d1ae0c42"
      ],
      "layout": "IPY_MODEL_d8579797fa02430f9d483e66294853e3"
     }
    },
    "d8579797fa02430f9d483e66294853e3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e3a3780722854663bc2efb382bacba8c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ea8aee72bab8429ab1801efc416ac974": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f1e45bee064b4b56989b73428196eb83",
      "max": 100,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_82ce1ce4042646a688c26139f2e8b726",
      "value": 100
     }
    },
    "ed0a082cfb25476ca1ce7e2d97280113": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f1e45bee064b4b56989b73428196eb83": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
